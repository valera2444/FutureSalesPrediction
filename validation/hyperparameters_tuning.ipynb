{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valeriy/python_envs/predict_future_sales/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from collections import defaultdict\n",
    "\n",
    "import optuna\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import time\n",
    "\n",
    "#np.random.seed(42)\n",
    "\n",
    "SOURCE_PATH = 'data/merged.csv'\n",
    "def prepare_past_ID_s(data_train):\n",
    "    data_train['shop_item'] = [tuple([shop, item]) for shop, item in zip(data_train['shop_id'], data_train['item_id'])]\n",
    "    #34 block contains A LOT more shop_item than others\n",
    "    shop_item_pairs_in_dbn = data_train.groupby('date_block_num')['shop_item'].apply(np.unique)\n",
    "    data_train = data_train.drop(['shop_item'], axis=1)\n",
    "    \n",
    "    shop_item_pairs_WITH_PREV_in_dbn = shop_item_pairs_in_dbn.copy()\n",
    "    \n",
    "    print(np.array(shop_item_pairs_WITH_PREV_in_dbn.index))\n",
    "    arr = np.array(shop_item_pairs_WITH_PREV_in_dbn.index)\n",
    "    \n",
    "    for block in arr[arr>=0]:\n",
    "        if block == 0:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        arr = np.append(shop_item_pairs_WITH_PREV_in_dbn[block -1],\n",
    "                                                            shop_item_pairs_in_dbn[block-1])\n",
    "        \n",
    "        \n",
    "        shop_item_pairs_WITH_PREV_in_dbn[block] = np.unique(np.append(shop_item_pairs_WITH_PREV_in_dbn[block -1],\n",
    "                                                            shop_item_pairs_in_dbn[block-1]))\n",
    "        print(len(shop_item_pairs_WITH_PREV_in_dbn[block]))\n",
    "\n",
    "    return shop_item_pairs_in_dbn, shop_item_pairs_WITH_PREV_in_dbn\n",
    "\n",
    "def prepare_past_ID_s_CARTESIAN(data_train):\n",
    "    data_train['shop_item'] = [tuple([shop, item]) for shop, item in zip(data_train['shop_id'], data_train['item_id'])]\n",
    "    #34 block contains A LOT more shop_item than others\n",
    "    shop_item_pairs_in_dbn = data_train.groupby('date_block_num')['shop_item'].apply(np.unique)\n",
    "    data_train = data_train.drop(['shop_item'], axis=1)\n",
    "    \n",
    "    shop_item_pairs_WITH_PREV_in_dbn = np.array([None] * len(shop_item_pairs_in_dbn))\n",
    "    \n",
    "    #print(np.array(shop_item_pairs_WITH_PREV_in_dbn.index))\n",
    "    \n",
    "\n",
    "    cartesians = []\n",
    "    for dbn in shop_item_pairs_in_dbn.index:\n",
    "        val = shop_item_pairs_in_dbn[dbn]\n",
    "\n",
    "        shops = np.unique(list(zip(*val))[0])\n",
    "        items = np.unique(list(zip(*val))[1])\n",
    "    \n",
    "        cartesian_product = np.random.permutation (np.array(np.meshgrid(shops, items)).T.reshape(-1, 2))\n",
    "        #print(cartesian_product)\n",
    "        cartesians.append(cartesian_product)\n",
    "        \n",
    "    \n",
    "    shop_item_pairs_WITH_PREV_in_dbn[0] = cartesians[0]\n",
    "    \n",
    "    for block in shop_item_pairs_in_dbn.index:\n",
    "        if block == 0:\n",
    "            continue\n",
    "        arr = np.append(shop_item_pairs_WITH_PREV_in_dbn[block - 1],\n",
    "                             cartesians[block - 1], axis=0)#shop_item_pairs_WITH_PREV_in_dbn doesnt contain 34 month\n",
    "        \n",
    "        shop_item_pairs_WITH_PREV_in_dbn[block] = np.unique(arr, axis=0)\n",
    "        print(len(shop_item_pairs_WITH_PREV_in_dbn[block]))\n",
    "    return shop_item_pairs_in_dbn, shop_item_pairs_WITH_PREV_in_dbn\n",
    "\n",
    "def make_X_lag_format(data, dbn):\n",
    "    \"\"\"\n",
    "    transform X to lag format\n",
    "    columns with dbn in names become lag_0, dbn-1 - lag_1 etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    lag_cols = defaultdict()\n",
    "    for col in data.columns:\n",
    "        splitted = col.split('$')\n",
    "        if len(splitted) == 1:\n",
    "            continue\n",
    "        \n",
    "        lag_cols[col] = splitted[0] + '_lag;' + str(dbn - int(splitted[1]))\n",
    "\n",
    "    #print(lag_cols)\n",
    "    data = data.rename(columns=dict(lag_cols))\n",
    "    #print(data.columns)\n",
    "    return data\n",
    "\n",
    "def prepare_train(data, valid ):\n",
    "    \"\"\"\n",
    "    returns one batch of merged data with required IDs from valid\n",
    "    \"\"\"\n",
    "    #print(data)\n",
    "    valid_shop_item = valid\n",
    "    valid_shop_item = list(zip(*valid_shop_item))\n",
    "    df = pd.DataFrame({'item_id':valid_shop_item[1],'shop_id':valid_shop_item[0]} )\n",
    "    data = df.merge(data, on=['shop_id','item_id'], how='inner').fillna(0)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_val(data, valid ):\n",
    "    \"\"\"\n",
    "    returns one batch of merged data with required IDs from valid\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame({'item_id':valid[:,1],'shop_id':valid[:,0]} )\n",
    "    data = df.merge(data, on=['shop_id','item_id'], how='inner').fillna(0)\n",
    "    #print('prepare_val, data:',len(data))\n",
    "    return data\n",
    "\n",
    "def prepare_data_train_boosting(data, valid, dbn):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    train = prepare_train (data, valid)\n",
    "    lag_cols = []\n",
    "    for col in data.columns:\n",
    "        \n",
    "        splitted = col.split('$')\n",
    "        if len(splitted) == 1:\n",
    "                lag_cols.append(col)\n",
    "                continue\n",
    "        #if 'shop_item_cnt' not in col:\n",
    "        #    continue\n",
    "            \n",
    "        for db in range(0,dbn-1):\n",
    "            \n",
    "            if db == int(splitted[1]):\n",
    "                #print(col)\n",
    "                lag_cols.append(col)\n",
    "\n",
    "    #print(lag_cols)\n",
    "    X = train[lag_cols]\n",
    "    Y = train[f'value_shop_id_item_id${dbn-1}']\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def prepare_data_validation_boosting(data, valid, dbn):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    test = prepare_val (data, valid)\n",
    "    \n",
    "    lag_cols = []\n",
    "    for col in test.columns:\n",
    "        \n",
    "            \n",
    "        splitted = col.split('$')\n",
    "        if len(splitted) == 1:\n",
    "                lag_cols.append(col)\n",
    "                continue\n",
    "        #if 'shop_item_cnt' not in col:\n",
    "        #    continue\n",
    "        for db in range(1,dbn):\n",
    "            \n",
    "            if db == int(splitted[1]):\n",
    "                #print(db, int(''.join(re.findall(r'\\d+', col))))\n",
    "                lag_cols.append(col)\n",
    "\n",
    "    X = test[lag_cols]\n",
    "    Y = test[f'value_shop_id_item_id${dbn}']#value_shop_id_item_id\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def select_columns_for_reading(path, dbn):\n",
    "   \n",
    "    columns = pd.read_csv(path, nrows=0).columns.tolist()\n",
    "\n",
    "    cols = []\n",
    "    for col in columns:\n",
    "        l = col.split('$')\n",
    "        if len(l) == 1:\n",
    "            cols.append(col)\n",
    "            continue\n",
    "\n",
    "        name = l[0]\n",
    "        num=int(l[1])\n",
    "        dbn_diff = dbn - num\n",
    "        \n",
    "        if 'value_shop_id_item_id' in col and np.isclose(dbn_diff,0):#target\n",
    "            cols.append(col)\n",
    "\n",
    "        if dbn_diff<=0:\n",
    "            continue\n",
    "\n",
    "\n",
    "        if 'ema' in name and dbn_diff <= 3:\n",
    "            cols.append(col)\n",
    "            continue\n",
    "        elif 'value_shop_id_item_id' in name and (dbn_diff <= 6 or dbn_diff == 12):\n",
    "            cols.append(col)\n",
    "            continue\n",
    "        elif 'value_price' in name and dbn_diff <= 1:\n",
    "            cols.append(col)\n",
    "            continue\n",
    "        elif 'value' in name and dbn_diff <= 3:\n",
    "            cols.append(col)\n",
    "            continue\n",
    "        elif 'diff' in name and dbn_diff == 1:\n",
    "            cols.append(col)\n",
    "            continue\n",
    "        elif 'change' in name and dbn_diff <= 2:\n",
    "            cols.append(col)\n",
    "            continue\n",
    "\n",
    "\n",
    "    return cols\n",
    "\n",
    "def create_batch_train(batch_size, dbn, shop_item_pairs_WITH_PREV_in_dbn, batch_size_to_read):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    train = np.random.permutation (shop_item_pairs_WITH_PREV_in_dbn[dbn])#-1?????????\n",
    "\n",
    "    #chunk_num =  len(train)// batch_size if len(train)%batch_size==0  else   len(train) // batch_size + 1#MAY BE NEED TO CORRECT\n",
    "    chunk_num =  len(train)// batch_size if len(train)>=batch_size else 1#MAY BE NEED TO CORRECT\n",
    "    columns = select_columns_for_reading(SOURCE_PATH, dbn-1)#-1?????\n",
    "    for idx in range(chunk_num):#split shop_item_pairs_WITH_PREV_in_dbn into chuncks\n",
    "        t1 = time.time()\n",
    "        l_x=[]\n",
    "        l_y=[]\n",
    "        merged = pd.read_csv(SOURCE_PATH, chunksize=batch_size_to_read, skipinitialspace=True, usecols=columns)\n",
    "        l_sum = 0\n",
    "        for chunck in merged:#split merged into chuncks\n",
    "            \n",
    "            l =  prepare_data_train_boosting(chunck,train[idx*batch_size:(idx+1)*batch_size], dbn) \n",
    "            #print(len(l[0]))\n",
    "            l_sum += len(l[0])\n",
    "            l_x.append( l[0] )\n",
    "            l_y. append(l[1])\n",
    "        \n",
    "        if len(l_x) == 0:\n",
    "            return [None, None]\n",
    "        print('create_batch_train, 203:',l_sum)\n",
    "        l_x = pd.concat(l_x)\n",
    "        l_y = pd.concat(l_y)\n",
    "\n",
    "        t2 = time.time()\n",
    "        print('batch creation time [create_batch_train, 212],',t2-t1)\n",
    "        return [l_x, l_y]#, test\n",
    "\n",
    "def create_batch_val(batch_size, dbn, shop_item_pairs_in_dbn, batch_size_to_read):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    val = shop_item_pairs_in_dbn[dbn]\n",
    "    \n",
    "    shops = np.unique(list(zip(*val))[0])\n",
    "    items = np.unique(list(zip(*val))[1])\n",
    "\n",
    "    cartesian_product = np.random.permutation (np.array(np.meshgrid(shops, items)).T.reshape(-1, 2))\n",
    "    \n",
    "    chunk_num =  len(cartesian_product)// batch_size if len(cartesian_product)%batch_size==0  else   len(cartesian_product) // batch_size + 1#MAY BE NEED TO CORRECT\n",
    "\n",
    "    columns = select_columns_for_reading(SOURCE_PATH, dbn)\n",
    "\n",
    "\n",
    "    for idx in range(chunk_num):\n",
    "        merged = pd.read_csv(SOURCE_PATH, chunksize=batch_size_to_read, skipinitialspace=True, usecols=columns)\n",
    "        l_x=[]\n",
    "        l_y=[]\n",
    "        l_sum=0\n",
    "        cartesian = cartesian_product[idx*batch_size:(idx+1)*batch_size]\n",
    "\n",
    "        for chunck in merged:\n",
    "            #print('chunck',len(chunck))\n",
    "            #print('cartesian_product',len(cartesian))\n",
    "            \n",
    "            l =  prepare_data_validation_boosting(chunck,cartesian, dbn) \n",
    "            l_sum+=len(l[0])\n",
    "            l_x.append( l[0] )\n",
    "            l_y. append( l[1] )\n",
    "\n",
    "        if len(l_x) == 0:\n",
    "            return [None, None]\n",
    "        print('create_batch_val,243:',l_sum)\n",
    "        l_x = pd.concat(l_x)\n",
    "        l_y = pd.concat(l_y)\n",
    "\n",
    "        #print('prepare_batch_val, l_x:',l_x)\n",
    "        return [l_x,l_y]#, test\n",
    "\n",
    "\n",
    "def select_columns(X_train, dbn):#WHEN LINEAR MODELS, X_train = append_some_columns(X_train,dbn) - to comment\n",
    "    X_train = append_some_columns(X_train,dbn)\n",
    "    cols=[]\n",
    "    for col in X_train.columns:\n",
    "        l = col.split(';')\n",
    "        if len(l) == 1:\n",
    "            cols.append(col)\n",
    "            continue\n",
    "        name = l[0]\n",
    "        num = int(l[1])\n",
    "        if 'ema' in name:\n",
    "           if num <= 3:\n",
    "                cols.append(col)\n",
    "                continue\n",
    "        if 'value_shop_id_item_id' in name:\n",
    "            if num <=6 or num == 12:\n",
    "                cols.append(col)\n",
    "                continue\n",
    "        if 'value_shop_id_lag' in name:\n",
    "            continue\n",
    "\n",
    "        if 'value_price' in name:\n",
    "            if num <= 1:\n",
    "                cols.append(col)\n",
    "                continue\n",
    "            \n",
    "        if 'value' in name:\n",
    "            if num <=3:\n",
    "                cols.append(col)\n",
    "                continue\n",
    "            \n",
    "        if 'diff' in name:\n",
    "            if num == 1:\n",
    "                cols.append(col)\n",
    "                continue\n",
    "            continue\n",
    "            \n",
    "        if 'change' in name:\n",
    "\n",
    "            if num <= 2:\n",
    "                cols.append(col)\n",
    "                continue\n",
    "\n",
    "            continue\n",
    "        \n",
    "    return X_train[cols]\n",
    "\n",
    "def append_some_columns(X_train, dbn):\n",
    "    X_train['date_block_num'] = dbn\n",
    "    X_train['month'] = dbn%12\n",
    "    return X_train\n",
    "\n",
    "def train_model(model, batch_size, val_month, shop_item_pairs_WITH_PREV_in_dbn,batch_size_to_read,epochs,callback):\n",
    "    \n",
    "    first=True\n",
    "    columns_order=None\n",
    "    \n",
    "    Y_true_l = []\n",
    "    preds_l = []\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        X_train,Y_train  = create_batch_train(batch_size, val_month,shop_item_pairs_WITH_PREV_in_dbn,batch_size_to_read)\n",
    "        t1_batch = time.time()\n",
    "        Y_train = np.clip(Y_train,0,20)\n",
    "        X_train = make_X_lag_format(X_train, val_month-1)\n",
    "        columns_order=X_train.columns\n",
    "        model=model.fit(X_train, Y_train)\n",
    "            \n",
    "        y_train_pred = model.predict(X_train)  \n",
    "        \n",
    "        Y_true_l.append(Y_train)\n",
    "        preds_l.append(y_train_pred)\n",
    "        t2_batch = time.time()\n",
    "        print(f'train on batch {c} time,',t2_batch-t1_batch)\n",
    "        c+=1\n",
    "        \n",
    "    train_rmse = root_mean_squared_error(pd.concat(Y_true_l), np.concat(preds_l))\n",
    "    print('train_rmse, ',train_rmse)\n",
    "    return model, columns_order\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_ML(batch_size,val_monthes, shop_item_pairs_in_dbn, shop_item_pairs_WITH_PREV_in_dbn,batch_size_to_read,params):\n",
    "    \"\"\"\n",
    "    Function for validating model\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    val_errors = []\n",
    "    \n",
    "    val_preds=[]\n",
    "    \n",
    "    first=True\n",
    "    for val_month in val_monthes:\n",
    "        \"\"\"\n",
    "        optuna_callback = optuna.integration.XGBoostPruningCallback(params['trial'], observation_key='validation_1-rmse')\n",
    "        if first:\n",
    "            model = xgb.XGBRegressor(eta=params['lr'], \n",
    "                                    max_leaves=params['num_leaves'],\n",
    "                                    device='gpu', \n",
    "                                    enable_categorical=True,\n",
    "                                    n_estimators=params['n_estimators'],\n",
    "                                    callbacks=[optuna_callback],\n",
    "                                    verbosity=0)\n",
    "        \n",
    "        else:\n",
    "            model = xgb.XGBRegressor(eta=params['lr'], \n",
    "                                    max_leaves=params['num_leaves'],\n",
    "                                    device='gpu', \n",
    "                                    enable_categorical=True,\n",
    "                                    n_estimators=params['n_estimators'],\n",
    "                                    verbosity=0)\n",
    "        \"\"\"\n",
    "        optuna_callback = optuna.integration.LightGBMPruningCallback(params['trial'], valid_name='valid_1', metric='rmse')\n",
    "        model = LGBMRegressor(verbose=-1,n_jobs=7, num_leaves=params['num_leaves'], n_estimators = params['n_estimators'],  learning_rate=params['lr'], objective='rmse')             \n",
    "        \n",
    "        print(type(model))\n",
    "        \n",
    "        if type(model) == LGBMRegressor:\n",
    "            X_val, Y_val = create_batch_val(batch_size, val_month, shop_item_pairs_in_dbn, batch_size_to_read)#but then cartesian product used\n",
    "            Y_val = np.clip(Y_val,0,20)\n",
    "            X_val = make_X_lag_format(X_val, val_month)\n",
    "\n",
    "            X_train,Y_train  = create_batch_train(batch_size, val_month,shop_item_pairs_WITH_PREV_in_dbn,batch_size_to_read)\n",
    "            Y_train = np.clip(Y_train,0,20)\n",
    "            X_train = make_X_lag_format(X_train, val_month-1)\n",
    "            \n",
    "            columns_order = X_train.columns\n",
    "            X_val=X_val[columns_order]\n",
    "\n",
    "\n",
    "            X_val = X_val.drop('item_id', axis=1)\n",
    "            X_val['shop_id'] = X_val['shop_id'].astype('category')\n",
    "            X_val['item_category_id'] = X_val['item_category_id'].astype('category')\n",
    "            X_val['city'] = X_val['city'].astype('category')\n",
    "            X_val['super_category'] = X_val['super_category'].astype('category')\n",
    "\n",
    "            X_train = X_train.drop('item_id', axis=1)\n",
    "            X_train['shop_id'] = X_train['shop_id'].astype('category')\n",
    "            X_train['item_category_id'] = X_train['item_category_id'].astype('category')\n",
    "            X_train['city'] = X_train['city'].astype('category')\n",
    "            X_train['super_category'] = X_train['super_category'].astype('category')\n",
    "\n",
    "            if first:\n",
    "                model=model.fit(X_train, Y_train,eval_set=[(X_train, Y_train), \n",
    "                                                        (X_val, Y_val)],\n",
    "                                                        callbacks=[optuna_callback])\n",
    "                \n",
    "            else:\n",
    "                model=model.fit(X_train, Y_train,eval_set=[(X_train, Y_train), \n",
    "                                                        (X_val, Y_val)])\n",
    "                \n",
    "            print(model.best_score_)\n",
    "            val_errors.append(model.best_score_['valid_1']['rmse'])\n",
    "\n",
    "\n",
    "        else:\n",
    "            X_val, Y_val = create_batch_val(batch_size, val_month, shop_item_pairs_in_dbn, batch_size_to_read)#but then cartesian product used\n",
    "            Y_val = np.clip(Y_val,0,20)\n",
    "            X_val = make_X_lag_format(X_val, val_month)\n",
    "\n",
    "            X_train,Y_train  = create_batch_train(batch_size, val_month,shop_item_pairs_WITH_PREV_in_dbn,batch_size_to_read)\n",
    "            Y_train = np.clip(Y_train,0,20)\n",
    "            X_train = make_X_lag_format(X_train, val_month-1)\n",
    "            \n",
    "            columns_order = X_train.columns\n",
    "            X_val=X_val[columns_order]\n",
    "            model=model.fit(X_train, \n",
    "                            Y_train,\n",
    "                            eval_set=[(X_train, Y_train), (X_val, Y_val)],\n",
    "                            verbose=False)\n",
    "            \n",
    "            print(model.evals_result()['validation_1']['rmse'][-1])\n",
    "            val_errors.append(model.evals_result()['validation_1']['rmse'][-1])\n",
    "\n",
    "        first=False\n",
    "       \n",
    "        \n",
    "\n",
    "    return val_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364950\n",
      "428871\n",
      "466086\n",
      "494493\n",
      "532909\n",
      "566259\n",
      "587979\n",
      "609623\n",
      "627192\n",
      "664844\n",
      "686985\n",
      "719696\n",
      "730116\n",
      "746129\n",
      "775024\n",
      "799403\n",
      "814628\n",
      "828506\n",
      "851457\n",
      "871899\n",
      "890066\n",
      "928598\n",
      "952398\n",
      "976804\n",
      "987057\n",
      "997953\n",
      "1013772\n",
      "1025692\n",
      "1035736\n",
      "1046582\n",
      "1055558\n",
      "1067461\n",
      "1080188\n",
      "1110590\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv('../data_cleaned/data_train.csv')\n",
    "test = pd.read_csv('../data_cleaned/test.csv')\n",
    "test['date_block_num'] = 34\n",
    "data_train = pd.concat([data_train,test ], ignore_index=True).drop('ID', axis=1).fillna(0)\n",
    "\n",
    "\n",
    "shop_item_pairs_in_dbn, shop_item_pairs_WITH_PREV_in_dbn = prepare_past_ID_s_CARTESIAN(data_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size=70000000\n",
    "batch_size_to_read=200000000\n",
    "epochs=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_errors = open('errors.txt','w')\n",
    "def objective(trial,val_monthes):\n",
    "\n",
    "    lr = trial.suggest_float('lr', low=0.005, high = 0.5,log=True)\n",
    "    num_leaves = trial.suggest_int('num_leaves', low=32, high = 1024,step=128)\n",
    "    n_estimators=trial.suggest_int('n_estimators', low=150, high = 1000,step=150)\n",
    "\n",
    "    print('validation started...')\n",
    "    params = defaultdict()\n",
    "    params['trial'] = trial\n",
    "    params['lr'] = lr\n",
    "    params['num_leaves'] = num_leaves\n",
    "    params['n_estimators'] = n_estimators\n",
    "    \n",
    "    \n",
    "    val_errors = validate_ML(\n",
    "                            batch_size=batch_size,\n",
    "                            val_monthes=val_monthes, \n",
    "                            shop_item_pairs_in_dbn=shop_item_pairs_in_dbn,\n",
    "                            shop_item_pairs_WITH_PREV_in_dbn=shop_item_pairs_WITH_PREV_in_dbn,\n",
    "                            batch_size_to_read=batch_size_to_read,\n",
    "                            params=params\n",
    "\n",
    "    )\n",
    "\n",
    "    print('lr:'+str(lr) + ' ' + str(val_errors) + '\\n')\n",
    "    file_errors.write('lr:'+str(lr) + ' ' + str(val_errors) + '\\n')\n",
    "    return np.mean(val_errors)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./microlog.txt','w') as log:\n",
    "    val_monthes=[23,26,33]\n",
    "    study_name = \"example-study\"  # Unique identifier of th\n",
    "    storage_name = \"sqlite:///{}.db\".format(study_name)\n",
    "    objective = partial(objective, val_monthes = val_monthes)\n",
    "    optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(log))\n",
    "\n",
    "    pruner=optuna.pruners.HyperbandPruner()\n",
    "    pruner=optuna.pruners.PatientPruner(pruner, patience=20)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\",pruner=pruner, storage=storage_name, load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_errors.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9055736407698162)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean([np.float64(1.0131770809048026), np.float64(1.213764768131793), np.float64(1.1784909782148614), np.float64(0.8400212244494581), np.float64(0.7838528695385018), np.float64(0.8931294069791396), np.float64(0.8026388507250694), np.float64(0.8118301797125215), np.float64(0.7120719933561963), np.float64(0.7786653890686942), np.float64(0.8994948504784775), np.float64(0.939746097678279)])\n",
    "#LGBM after optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9076828001933349)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([\n",
    "0.9964632349210584,\n",
    "0.837643588005918,\n",
    "0.7254309603751931,\n",
    "\n",
    "\n",
    "1.226628282051866,\n",
    "1.0650545693434619,\n",
    "0.8028916203328462,\n",
    "0.9080615108545917,\n",
    "0.849627754340868,\n",
    "0.8303376953162739,\n",
    "0.7867886964719496,\n",
    "0.8979238108194684,\n",
    "0.9653418794865235])\n",
    "\n",
    "#xgboost after optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_future_sales",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
