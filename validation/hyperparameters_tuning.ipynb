{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valeriy/python_envs/predict_future_sales/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from collections import defaultdict\n",
    "import optuna\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import time\n",
    "\n",
    "#np.random.seed(42)\n",
    "\n",
    "SOURCE_PATH = 'data/merged.csv'\n",
    "def prepare_past_ID_s(data_train):\n",
    "    \"\"\"\n",
    "    This function doesn't used\n",
    "    \"\"\"\n",
    "    data_train['shop_item'] = [tuple([shop, item]) for shop, item in zip(data_train['shop_id'], data_train['item_id'])]\n",
    "    #34 block contains A LOT more shop_item than others\n",
    "    shop_item_pairs_in_dbn = data_train.groupby('date_block_num')['shop_item'].apply(np.unique)\n",
    "    data_train = data_train.drop(['shop_item'], axis=1)\n",
    "    \n",
    "    shop_item_pairs_WITH_PREV_in_dbn = shop_item_pairs_in_dbn.copy()\n",
    "    \n",
    "    print(np.array(shop_item_pairs_WITH_PREV_in_dbn.index))\n",
    "    arr = np.array(shop_item_pairs_WITH_PREV_in_dbn.index)\n",
    "    \n",
    "    for block in arr[arr>=0]:\n",
    "        if block == 0:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        arr = np.append(shop_item_pairs_WITH_PREV_in_dbn[block -1],\n",
    "                                                            shop_item_pairs_in_dbn[block-1])\n",
    "        \n",
    "        \n",
    "        shop_item_pairs_WITH_PREV_in_dbn[block] = np.unique(np.append(shop_item_pairs_WITH_PREV_in_dbn[block -1],\n",
    "                                                            shop_item_pairs_in_dbn[block-1]))\n",
    "        print(len(shop_item_pairs_WITH_PREV_in_dbn[block]))\n",
    "\n",
    "   \n",
    "    return shop_item_pairs_in_dbn, shop_item_pairs_WITH_PREV_in_dbn\n",
    "\n",
    "def prepare_past_ID_s_CARTESIAN(data_train):\n",
    "    \"\"\"\n",
    "    Prepares unique (shop, item) pairs in a Cartesian product format over time blocks.\n",
    "    \n",
    "    Args:\n",
    "        data_train (pd.DataFrame): Training data with 'shop_id', 'item_id', and 'date_block_num' columns.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - shop_item_pairs_in_dbn (pd.DataFrame): Cartesian product of shop_id and item_id columns from data_train for each 'date_block_num'.\n",
    "            - shop_item_pairs_WITH_PREV_in_dbn:np.array[np.array[np.array[int]]] Accumulated cartesian products for each time block up since 0 to the previous block. This name may confuse, as it contains only previous information\n",
    "            \n",
    "    \"\"\"\n",
    "    data_train['shop_item'] = [tuple([shop, item]) for shop, item in zip(data_train['shop_id'], data_train['item_id'])]\n",
    "    #34 block contains A LOT more shop_item than others\n",
    "    shop_item_pairs_in_dbn = data_train.groupby('date_block_num')['shop_item'].apply(np.unique)\n",
    "    data_train = data_train.drop(['shop_item'], axis=1)\n",
    "    \n",
    "    shop_item_pairs_WITH_PREV_in_dbn = np.array([None] * len(shop_item_pairs_in_dbn))\n",
    "    \n",
    "    #print(np.array(shop_item_pairs_WITH_PREV_in_dbn.index))\n",
    "    \n",
    "\n",
    "    cartesians = []\n",
    "    for dbn in shop_item_pairs_in_dbn.index:\n",
    "        val = shop_item_pairs_in_dbn[dbn]\n",
    "\n",
    "        shops = np.unique(list(zip(*val))[0])\n",
    "        items = np.unique(list(zip(*val))[1])\n",
    "    \n",
    "        cartesian_product = np.random.permutation (np.array(np.meshgrid(shops, items)).T.reshape(-1, 2))\n",
    "        #print(cartesian_product)\n",
    "        cartesians.append(cartesian_product)\n",
    "        \n",
    "    \n",
    "    shop_item_pairs_WITH_PREV_in_dbn[0] = cartesians[0]\n",
    "    \n",
    "    for block in shop_item_pairs_in_dbn.index:\n",
    "        if block == 0:\n",
    "            continue\n",
    "        arr = np.append(shop_item_pairs_WITH_PREV_in_dbn[block - 1],\n",
    "                             cartesians[block - 1], axis=0)#shop_item_pairs_WITH_PREV_in_dbn doesnt contain 34 month\n",
    "        \n",
    "        shop_item_pairs_WITH_PREV_in_dbn[block] = np.unique(arr, axis=0)\n",
    "        print(len(shop_item_pairs_WITH_PREV_in_dbn[block]))\n",
    "\n",
    "    for i in range(len(shop_item_pairs_WITH_PREV_in_dbn)):\n",
    "        shop_item_pairs_WITH_PREV_in_dbn[i] = np.random.permutation(shop_item_pairs_WITH_PREV_in_dbn[i])\n",
    "    return shop_item_pairs_in_dbn, shop_item_pairs_WITH_PREV_in_dbn\n",
    "\n",
    "def make_X_lag_format(data, dbn):\n",
    "    \"\"\"\n",
    "    Converts columns with date block numbers to a lag format for specified date block number.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Data containing columns with date block numbers. Columns must have format name$dbn\n",
    "        dbn (int): Current date block number for calculating lags.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Data with lagged columns for the specified date block number. Columns will have format name_lag;{lag number}\n",
    "    \"\"\"\n",
    "    \n",
    "    lag_cols = defaultdict()\n",
    "    for col in data.columns:\n",
    "        splitted = col.split('$')\n",
    "        if len(splitted) == 1:\n",
    "            continue\n",
    "        \n",
    "        lag_cols[col] = splitted[0] + '_lag;' + str(dbn - int(splitted[1]))\n",
    "\n",
    "    #print(lag_cols)\n",
    "    data = data.rename(columns=dict(lag_cols))\n",
    "    #print(data.columns)\n",
    "    return data\n",
    "\n",
    "def prepare_train(data, valid ):\n",
    "    \"\"\"\n",
    "    Filters training data to include only the specified shop-item pairs.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Training data to be filtered.\n",
    "        valid np.array[np.array[int]]: shop, item pairs to include in a batch.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered data with only specified shop-item pairs.\n",
    "    \"\"\"\n",
    "    #print(data)\n",
    "    valid_shop_item = valid\n",
    "    valid_shop_item = list(zip(*valid_shop_item))\n",
    "    df = pd.DataFrame({'item_id':valid_shop_item[1],'shop_id':valid_shop_item[0]} )\n",
    "    data = df.merge(data, on=['shop_id','item_id'], how='inner').fillna(0)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_val(data, valid ):\n",
    "    \"\"\"\n",
    "    Filters validation data to include only specified shop-item pairs.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Validation data to be filtered.\n",
    "        valid (np.array[np.array[int]]): shop, item pairs to include in a batch.\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered data for validation with only specified shop-item pairs.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame({'item_id':valid[:,1],'shop_id':valid[:,0]} )\n",
    "    data = df.merge(data, on=['shop_id','item_id'], how='inner').fillna(0)\n",
    "    #print('prepare_val, data:',len(data))\n",
    "    return data\n",
    "\n",
    "def prepare_data_train_boosting(data, valid, dbn):\n",
    "    \"\"\"\n",
    "    Prepares validation data for boosting models by selecting required columns and selecting only required (shop,item) pairs\n",
    "    This function was used before reading only part of columns from csv, but still be used to validate everything is right.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Training data.\n",
    "        valid (np.array[np.array[int]]): shop, item pairs to include in a batch.\n",
    "        dbn (int): Current date block number.\n",
    "\n",
    "    Returns:\n",
    "        tuple: \n",
    "            - X (pd.DataFrame): Features for training.\n",
    "            - Y (pd.Series): Target variable for training.\n",
    "    \"\"\"\n",
    "    train = prepare_train (data, valid)\n",
    "    lag_cols = []\n",
    "    for col in data.columns:\n",
    "        \n",
    "        splitted = col.split('$')\n",
    "        if len(splitted) == 1:\n",
    "                lag_cols.append(col)\n",
    "                continue\n",
    "        #if 'shop_item_cnt' not in col:\n",
    "        #    continue\n",
    "            \n",
    "        for db in range(0,dbn-1):\n",
    "            \n",
    "            if db == int(splitted[1]):\n",
    "                #print(col)\n",
    "                lag_cols.append(col)\n",
    "\n",
    "    #print(lag_cols)\n",
    "    X = train[lag_cols]\n",
    "    Y = train[f'value_shop_id_item_id${dbn-1}']\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def prepare_data_validation_boosting(data, valid, dbn):\n",
    "    \"\"\"\n",
    "    Prepares validation data for boosting models by selecting required columns and selecting only required (shop,item) pairs\n",
    "    This function was used before reading only part of columns from csv, but still be used to validate everything is right.\n",
    "    Args:\n",
    "        data (pd.DataFrame): Validation data.\n",
    "        valid (np.array[np.array[int]]): shop, item pairs to include in a batch.\n",
    "        dbn (int): Current date block number.\n",
    "\n",
    "    Returns:\n",
    "        tuple: \n",
    "            - X (pd.DataFrame): Features for validation.\n",
    "            - Y (pd.Series): Target variable for validation.\n",
    "    \"\"\"\n",
    "    test = prepare_val (data, valid)\n",
    "    \n",
    "    lag_cols = []\n",
    "    for col in test.columns:\n",
    "        \n",
    "            \n",
    "        splitted = col.split('$')\n",
    "        if len(splitted) == 1:\n",
    "                lag_cols.append(col)\n",
    "                continue\n",
    "        for db in range(1,dbn):\n",
    "            \n",
    "            if db == int(splitted[1]):\n",
    "                #print(db, int(''.join(re.findall(r'\\d+', col))))\n",
    "                lag_cols.append(col)\n",
    "\n",
    "    X = test[lag_cols]\n",
    "    Y = test[f'value_shop_id_item_id${dbn}']\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def select_columns_for_reading(path, dbn):\n",
    "    \"\"\"\n",
    "    Selects relevant columns to read from a CSV file based on the date block number. Feature selection step done here\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the CSV file.\n",
    "        dbn (int): Date block number used to filter columns.\n",
    "\n",
    "    Returns:\n",
    "        list: List of column names to read.\n",
    "    \"\"\"\n",
    "    columns = pd.read_csv(path, nrows=0).columns.tolist()\n",
    "\n",
    "    cols = []\n",
    "    for col in columns:\n",
    "        l = col.split('$')\n",
    "        if len(l) == 1:\n",
    "            cols.append(col)\n",
    "            continue\n",
    "\n",
    "        name = l[0]\n",
    "        num=int(l[1])\n",
    "        dbn_diff = dbn - num\n",
    "        \n",
    "        if 'value_shop_id_item_id' in col and np.isclose(dbn_diff,0):#target\n",
    "            cols.append(col)\n",
    "\n",
    "        if dbn_diff<=0:\n",
    "            continue\n",
    "\n",
    "\n",
    "        if 'ema' in name and dbn_diff <= 3:\n",
    "            cols.append(col)\n",
    "            continue\n",
    "        elif 'value_shop_id_item_id' in name and (dbn_diff <= 3 or dbn_diff == 6 or dbn_diff == 12):\n",
    "            cols.append(col)\n",
    "            continue\n",
    "        elif 'value_price' in name and dbn_diff <= 1:\n",
    "            cols.append(col)\n",
    "            continue\n",
    "        elif 'value' in name and dbn_diff <= 3:\n",
    "            cols.append(col)\n",
    "            continue\n",
    "        elif 'diff' in name and dbn_diff == 1:\n",
    "            cols.append(col)\n",
    "            continue\n",
    "        elif 'change' in name and dbn_diff <= 2:\n",
    "            cols.append(col)\n",
    "            continue\n",
    "\n",
    "\n",
    "    return cols\n",
    "\n",
    "def sample_indexes(shop_item_pairs_WITH_PREV_in_dbn, number_of_batches):\n",
    "    \"\"\"\n",
    "    Samples indexes for batch learning. Indexes must be applied on shop_item_pairs_WITH_PREV_in_dbn\n",
    "    Args:\n",
    "        shop_item_pairs_WITH_PREV_in_dbn ( np.array[np.array[np.array[int]]] ): array of accumulated cartesian products of (shop, item) for date_block_nums\n",
    "        batch_size (int): batch size for lgbm\n",
    "        number_of_batches (int): number of batches for lgbm,\n",
    "\n",
    "    Returns:\n",
    "        list[list[tuple]]: list of batch indexes for each date_block_num. \n",
    "    \"\"\"\n",
    "    \n",
    "    lengthes = np.array(list(map(len, shop_item_pairs_WITH_PREV_in_dbn)))\n",
    "\n",
    "    batch_size_in_dbn =lengthes // number_of_batches\n",
    "    #print(lengthes)\n",
    "\n",
    "    to_ret=[]#this will contain last_monthes_to_take_in_train array, where each array contains split for batches for some dbn\n",
    "    for bs in batch_size_in_dbn:\n",
    "        idxs_in_dbn = []\n",
    "        for i in range(number_of_batches):\n",
    "            idxs_in_dbn.append((i*bs,(i+1)*bs))\n",
    "        #print(np.max(idxs_in_dbn))\n",
    "        to_ret.append(idxs_in_dbn)\n",
    "\n",
    "    return to_ret\n",
    "\n",
    "def create_batch_train(batch_size, dbn, shop_item_pairs_WITH_PREV_in_dbn, batch_size_to_read):\n",
    "    \"\"\"\n",
    "    Creates training batches for date_block_num.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        dbn (int): Date block number.\n",
    "        shop_item_pairs_WITH_PREV_in_dbn (np.array[np.array[np.array[int]]]):  Accumulated cartesian products for each time block.\n",
    "        batch_size_to_read (int): Chunk size for reading data from csv.\n",
    "\n",
    "    Yields:\n",
    "        tuple: \n",
    "            - X (pd.DataFrame): Feature batch.\n",
    "            - Y (pd.Series): Target batch.\n",
    "    \"\"\"\n",
    "    last_monthes_to_take_in_train = 14\n",
    "    lengthes = np.array(list(map(len, shop_item_pairs_WITH_PREV_in_dbn))) \n",
    "    \n",
    "    total_number_of_samples=sum(lengthes[dbn-last_monthes_to_take_in_train+1:dbn+1])\n",
    "    number_of_batches = total_number_of_samples // batch_size if batch_size <= total_number_of_samples else 1\n",
    "\n",
    "    #This variable will store batch indexes for date_block_nums [ dbn-last_monthes_to_take_in_train+1 , dbn]\n",
    "    idxs = sample_indexes(shop_item_pairs_WITH_PREV_in_dbn[dbn-last_monthes_to_take_in_train+1:dbn+1],number_of_batches)\n",
    "    print('total batches,',number_of_batches)\n",
    "    for batch_number in range(number_of_batches):\n",
    "        dbns = []\n",
    "        batch = []\n",
    "        l_x=[]\n",
    "        l_y=[]\n",
    "        t1 = time.time()\n",
    "        #This loop enables to includa data from different monthes in one batch\n",
    "        for dbn_inner in range(last_monthes_to_take_in_train):\n",
    "            curr_dbn = ((dbn_inner+1) - last_monthes_to_take_in_train) + dbn # in [dbn-last_monthes_to_take_in_train+1,dbn]\n",
    "            #idxs[dbn_inner] contains batch indexes for dbn_inner block(this array contains elements for [dbn-last_monthes_to_take_in_train+1,dbn] date_block_nums\n",
    "            \n",
    "            train = shop_item_pairs_WITH_PREV_in_dbn[curr_dbn][idxs[dbn_inner][batch_number][0] : idxs[dbn_inner][batch_number][1] ]\n",
    "\n",
    "            \n",
    "            columns = select_columns_for_reading(SOURCE_PATH,curr_dbn-1)\n",
    "            #(dbn-1) because this dbn is for validation, dbn for train is 1 less\n",
    "            merged = pd.read_csv('data/merged.csv', skipinitialspace=True, usecols=columns)\n",
    "            \n",
    "            \n",
    "\n",
    "            l_sum = 0\n",
    "            \n",
    "            l =  prepare_data_train_boosting(merged,train,curr_dbn) \n",
    "            l_sum += len(l[0])\n",
    "            l_0 = make_X_lag_format(l[0], curr_dbn-1)#-1 because this dbn is for validation, dbn for train is 1 less\n",
    "\n",
    "            l_0=append_some_columns(l_0, curr_dbn-1)\n",
    "\n",
    "            l_x.append( l_0 )\n",
    "            l_y. append(l[1])\n",
    "            \n",
    "            if len(l_x) == 0:\n",
    "                yield [None, None]\n",
    "\n",
    "            #print(f'create_batch_train, dbn {curr_dbn}:',l_sum)\n",
    "        t2 = time.time()\n",
    "        print('batch creation time [create_batch_train, 212],',t2-t1)\n",
    "                \n",
    "        l_x = pd.concat(l_x)\n",
    "        l_y = pd.concat(l_y)\n",
    "        print('batch size', len(l_x))\n",
    "        print(f'batch {batch_number} memory usage',np.sum(l_x.memory_usage()) / 10**6)\n",
    "        yield [l_x, l_y]#, test\n",
    "\n",
    "def create_batch_val(batch_size, dbn, shop_item_pairs_in_dbn, batch_size_to_read):\n",
    "    \"\"\"\n",
    "    Creates validation batches for date_block_num.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        dbn (int): Date block number.\n",
    "        shop_item_pairs_in_dbn (pd.DataFrame): dataframe of cartesian products of (shop, item) for date_block_nums\n",
    "        batch_size_to_read (int): Chunk size for reading data.\n",
    "\n",
    "    Yields:\n",
    "        tuple: \n",
    "            - X (pd.DataFrame): Feature batch.\n",
    "            - Y (pd.Series): Target batch.\n",
    "    \"\"\"\n",
    "    val = shop_item_pairs_in_dbn[dbn]\n",
    "    \n",
    "    shops = np.unique(list(zip(*val))[0])\n",
    "    items = np.unique(list(zip(*val))[1])\n",
    "\n",
    "    cartesian_product = np.random.permutation (np.array(np.meshgrid(shops, items)).T.reshape(-1, 2))\n",
    "    \n",
    "    chunk_num =  len(cartesian_product)// batch_size if len(cartesian_product)%batch_size==0  else   len(cartesian_product) // batch_size + 1#MAY BE NEED TO CORRECT\n",
    "\n",
    "    columns = select_columns_for_reading(SOURCE_PATH, dbn)\n",
    "\n",
    "\n",
    "    for idx in range(chunk_num):\n",
    "        merged = pd.read_csv('data/merged.csv', chunksize=batch_size_to_read, skipinitialspace=True, usecols=columns)\n",
    "        l_x=[]\n",
    "        l_y=[]\n",
    "        l_sum=0\n",
    "        cartesian = cartesian_product[idx*batch_size:(idx+1)*batch_size]\n",
    "\n",
    "        for chunck in merged:\n",
    "\n",
    "            l =  prepare_data_validation_boosting(chunck,cartesian, dbn) \n",
    "            l_sum+=len(l[0])\n",
    "            l_x.append( l[0] )\n",
    "            l_y. append( l[1] )\n",
    "\n",
    "        if len(l_x) == 0:\n",
    "            yield [None, None]\n",
    "        print('create_batch_val,243:',l_sum)\n",
    "        l_x = pd.concat(l_x)\n",
    "        l_y = pd.concat(l_y)\n",
    "\n",
    "\n",
    "        yield [l_x,l_y]#, test\n",
    "\n",
    "\n",
    "def append_some_columns(X_train, dbn):\n",
    "    \"\"\"\n",
    "    Adds additional columns like date_block_num and month to the training data.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training data.\n",
    "        dbn (int): Current date block number.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Training data with additional columns.\n",
    "    \"\"\"\n",
    "    X_train['date_block_num'] = dbn\n",
    "    X_train['month'] = dbn%12\n",
    "    return X_train\n",
    "#IMPORTANT: now model is trained =only on first batch\n",
    "def train_model(model, batch_size, val_month, shop_item_pairs_WITH_PREV_in_dbn,batch_size_to_read,epochs,shop_item_pairs_in_dbn,batches_for_training):\n",
    "    \"\"\"\n",
    "    Trains a machine learning model with specified batches and tracks RMSE for training on current val_month.\n",
    "    Args:\n",
    "        model (object): Machine learning model to be trained.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        val_month (int): Month used for validation.\n",
    "        shop_item_pairs_WITH_PREV_in_dbn (np.array[np.array[np.array[int,int]]]):  Accumulated cartesian products for each time block.\n",
    "        batch_size_to_read (int): Chunk size for reading data from csv.\n",
    "        epochs (int): Number of epochs for training. \n",
    "        shop_item_pairs_in_dbn (pd.DataFrame, optional): dataframe of cartesian products of (shop, item) for date_block_nums. If None, validation is not performed after every batch\n",
    "\n",
    "    Returns:\n",
    "        tuple: \n",
    "            - model (object): Trained model.\n",
    "            - columns_order (list[str]): Ordered list of feature columns.\n",
    "    \"\"\"\n",
    "    first=True\n",
    "    rmse = 0\n",
    "    c=0\n",
    "    columns_order=None\n",
    "    \n",
    "    Y_true_l = []\n",
    "    preds_l = []\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch,',epoch)\n",
    "        for X_train,Y_train  in create_batch_train(batch_size, val_month,shop_item_pairs_WITH_PREV_in_dbn,batch_size_to_read):\n",
    "            #print(f'train on batch {c} started')\n",
    "            t1_batch = time.time()\n",
    "            t1_data_prep = time.time()\n",
    "            #print(f'data preparation on batch {c} started')\n",
    "            if X_train is None:\n",
    "                print('None')\n",
    "                continue\n",
    "\n",
    "            if type(model) in [Lasso,SVC]:\n",
    "                #print(X_train.columns)\n",
    "                X_train.drop('shop_id', inplace=True, axis=1) \n",
    "                X_train.drop('item_category_id', inplace=True, axis=1) \n",
    "                X_train.drop('item_id', inplace=True, axis=1)\n",
    "                X_train.drop('city', inplace=True, axis=1)\n",
    "                X_train.drop('shop_id', inplace=True, axis=1)\n",
    "            elif type(model) ==LGBMRegressor:\n",
    "                #print(list(X_train.columns))\n",
    "            \n",
    "                X_train = X_train.drop('item_id', axis=1)\n",
    "                X_train['shop_id'] = X_train['shop_id'].astype('category')\n",
    "                X_train['item_category_id'] = X_train['item_category_id'].astype('category')\n",
    "                X_train['city'] = X_train['city'].astype('category')\n",
    "                X_train['super_category'] = X_train['super_category'].astype('category')\n",
    "                \n",
    "                pass\n",
    "            \n",
    "            \n",
    "                \n",
    "            Y_train = np.clip(Y_train,0,20)\n",
    "            \n",
    "            if X_train.empty:\n",
    "                print('None')\n",
    "                continue\n",
    "            \n",
    "            #X_train = make_X_lag_format(X_train, val_month-1)#-1 because this dbn is for validation, dbn for train is 1 less\n",
    "            \n",
    "            #X_train=select_columns(X_train, val_month-1)\n",
    "            \n",
    "            \n",
    "            columns_order=X_train.columns\n",
    "\n",
    "            t2_data_prep = time.time()\n",
    "            #print(f'data preparation on batch {c} time:',t2_data_prep-t1_data_prep)\n",
    "            #print('model fitting started')\n",
    "            t1_fit = time.time()\n",
    "            if c == 0:\n",
    "                pass\n",
    "                #print('train columns')\n",
    "                #print(X_train.columns)\n",
    "            if type(model) in [Lasso,SVC]:\n",
    "                model.fit(X_train, Y_train)\n",
    "                y_train_pred = model.predict(X_train)\n",
    "            \n",
    "            elif type(model) == LGBMRegressor:\n",
    "                if first:\n",
    "                    model.fit(X_train, Y_train)\n",
    "                    first=False\n",
    "                else:\n",
    "                    model.fit(X_train, Y_train, init_model=model)\n",
    "                y_train_pred = model.predict(X_train, validate_features=True)\n",
    "\n",
    "            elif type(model) == xgb.XGBRegressor:\n",
    "                if first:\n",
    "                    model=model.fit(X_train, Y_train)\n",
    "                    first=False\n",
    "                else:\n",
    "                    print(model.get_booster())\n",
    "                    #Works not as expected\n",
    "                    model=model.fit(X_train, Y_train, xgb_model=model.get_booster())\n",
    "                    \n",
    "                    \n",
    "                y_train_pred = model.predict(X_train)  \n",
    "\n",
    "            elif type(model) == RandomForestRegressor:\n",
    "                model.fit(X_train, Y_train)\n",
    "                y_train_pred = model.predict(X_train)  \n",
    "            t2_fit = time.time()\n",
    "            #print(f'model fitting time on batch {c},',t2_fit - t1_fit)\n",
    "            \n",
    "            Y_true_l.append(Y_train)\n",
    "            preds_l.append(y_train_pred)\n",
    "            t2_batch = time.time()\n",
    "            print(f'train on batch {c} time,',t2_batch-t1_batch)\n",
    "            \n",
    "            if shop_item_pairs_in_dbn is not None:\n",
    "                val_pred, val_error = validate_model(model,batch_size, val_month,columns_order, shop_item_pairs_in_dbn,batch_size_to_read)\n",
    "                print(f'val score after batch {c}', val_error)\n",
    "\n",
    "            c+=1\n",
    "            if c == batches_for_training:\n",
    "                break\n",
    "        \n",
    "    train_rmse = root_mean_squared_error(pd.concat(Y_true_l), np.concat(preds_l))\n",
    "    print('train_rmse, ',train_rmse)\n",
    "           \n",
    "\n",
    "    return model, columns_order\n",
    "\n",
    "def validate_model(model,batch_size, val_month, columns_order, shop_item_pairs_in_dbn, batch_size_to_read):\n",
    "    \"\"\"\n",
    "    Validates the model and calculates RMSE on the validation set on the current val_month.\n",
    "\n",
    "    Args:\n",
    "        model (object): Machine learning model to be validated.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        val_month (int): Month used for validation.\n",
    "        columns_order (list[str]): Order of feature columns.\n",
    "        shop_item_pairs_in_dbn (pd.DataFrame): dataframe of cartesian products of (shop, item) for date_block_nums\n",
    "        batch_size_to_read (int): Chunk size for reading data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: \n",
    "            - val_preds (np.array[float]): Predictions for validation set.\n",
    "            - val_rmse (float): RMSE for validation set.\n",
    "    \"\"\"\n",
    "    rmse = 0\n",
    "    c=0\n",
    "    \n",
    "    val_preds = []\n",
    "    Y_true_l = []\n",
    "    preds_l = []\n",
    "    \n",
    "    for X_val, Y_val in create_batch_val(batch_size, val_month, shop_item_pairs_in_dbn, batch_size_to_read):#but then cartesian product used\n",
    "        if X_val is None:\n",
    "                    continue\n",
    "        \n",
    "        if type(model) in [sklearn.linear_model._coordinate_descent.Lasso,\n",
    "                          SVC]:\n",
    "            \n",
    "            X_val.drop('shop_id', inplace=True, axis=1) \n",
    "            X_val.drop('item_category_id', inplace=True, axis=1) \n",
    "            X_val.drop('item_id', inplace=True, axis=1)\n",
    "            X_val.drop('city', inplace=True, axis=1)\n",
    "            X_val.drop('shop_id', inplace=True, axis=1)\n",
    "            \n",
    "\n",
    "        elif type(model) ==LGBMRegressor:\n",
    "            \n",
    "            X_val = X_val.drop('item_id', axis=1)\n",
    "            X_val['shop_id'] = X_val['shop_id'].astype('category')\n",
    "            X_val['item_category_id'] = X_val['item_category_id'].astype('category')\n",
    "            X_val['city'] = X_val['city'].astype('category')\n",
    "            X_val['super_category'] = X_val['super_category'].astype('category')\n",
    "                    \n",
    "            pass\n",
    "            \n",
    "        \n",
    "            \n",
    "        Y_val = np.clip(Y_val,0,20)\n",
    "        \n",
    "        \n",
    "        X_val = make_X_lag_format(X_val, val_month)\n",
    "        \n",
    "        X_val=append_some_columns(X_val, val_month)\n",
    "        X_val = X_val[columns_order]\n",
    "\n",
    "        if type(model) in [Lasso,SVC]:\n",
    "            y_val_pred = model.predict(X_val)#lgb - validate features\n",
    "            \n",
    "        elif type(model) ==LGBMRegressor:\n",
    "            y_val_pred = model.predict(X_val, validate_features=True)#lgb - validate features\n",
    "\n",
    "        elif type(model) == xgb.XGBRegressor:\n",
    "            y_val_pred = model.predict(X_val)   \n",
    "\n",
    "        elif type(model) == RandomForestRegressor:\n",
    "            y_val_pred = model.predict(X_val)  \n",
    "\n",
    "        y_val_pred = np.clip(y_val_pred,0,20)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        preds_l.append(y_val_pred)\n",
    "        Y_true_l.append(Y_val)\n",
    "        \n",
    "        c+=1\n",
    "        \n",
    "        val_preds.append(y_val_pred)\n",
    "\n",
    "    \n",
    "    val_rmse = root_mean_squared_error(pd.concat(Y_true_l), np.concat(preds_l))\n",
    "    print('val rmse, ',val_rmse)\n",
    "\n",
    "    return val_preds, val_rmse\n",
    "\n",
    "def validate_ML(params,batch_size,val_monthes, shop_item_pairs_in_dbn, shop_item_pairs_WITH_PREV_in_dbn,batch_size_to_read,epochs,batches_for_training):\n",
    "    \"\"\"\n",
    "    Runs model training and validation across multiple months and computes RMSE for each month.\n",
    "    Note: epochs must be set to 1. Otherwise good results are not guaranteed\n",
    "    Note: batch learning works properly only for LGBMRegressor. Using another model with batch_size<=len(shop_item_pairs_WITH_PREV_in_dbn[dbn]) will lead to incorrect results\n",
    "    Args:\n",
    "        model (object): Machine learning model.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        val_monthes (list): List of months for validation.\n",
    "        shop_item_pairs_in_dbn (pd.DataFrame): dataframe of cartesian products of (shop, item) for date_block_nums\n",
    "        shop_item_pairs_WITH_PREV_in_dbn ( np.array[np.array[np.array[int,int]]] ): array of accumulated cartesian products of (shop, item) for date_block_nums\n",
    "        batch_size_to_read (int): Chunk size for reading data from csv.\n",
    "        epochs (int): Number of epochs for training.\n",
    "\n",
    "    Returns:\n",
    "        tuple: \n",
    "            - val_errors (list[np.array[float]]): List of RMSE values for each month.\n",
    "            - val_preds (list[float]): Predictions for validation set.\n",
    "    \"\"\"\n",
    "    \n",
    "    val_errors = []\n",
    "    \n",
    "    val_preds=[]\n",
    "    \n",
    "    \n",
    "    for val_month in val_monthes:\n",
    "\n",
    "        model = LGBMRegressor(n_estimators=params['n_estimators'], learning_rate=params['lr'] , num_leaves=params['num_leaves'],n_jobs=8)\n",
    "        \n",
    "        print(f'month {val_month} started')\n",
    "        t1 = time.time()\n",
    "        \n",
    "        print('month', val_month%12)\n",
    "\n",
    "        model,columns_order = train_model(model, batch_size, val_month, shop_item_pairs_WITH_PREV_in_dbn,batch_size_to_read,epochs,shop_item_pairs_in_dbn,batches_for_training)\n",
    "\n",
    "        t2=time.time()\n",
    "        print(f'model training on {val_month} time,',t2-t1)\n",
    "        print('feature importances, ')\n",
    "        print(list(model.feature_names_in_[np.argsort( model.feature_importances_)][::-1]))\n",
    "        \n",
    "        t1 = time.time()\n",
    "\n",
    "        val_pred, val_error = validate_model(model,batch_size, val_month,columns_order, shop_item_pairs_in_dbn,batch_size_to_read)\n",
    "        t2 = time.time()\n",
    "        print(f'validation time on month {val_month},',t2-t1)\n",
    "        val_errors.append(val_error)\n",
    "        val_preds.append(val_pred)\n",
    "        \n",
    "\n",
    "    return val_errors, val_preds\n",
    "\n",
    "def create_submission(model,batch_size, columns_order, shop_item_pairs_in_dbn,batch_size_to_read):\n",
    "    \"\"\"\n",
    "    Generates predictions for the test dataset and prepares a submission file.\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        model (object): Trained machine learning model.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        columns_order (list): Ordered list of feature columns.\n",
    "        shop_item_pairs_in_dbn (pd.DataFrame): dataframe of cartesian products of (shop, item) for date_block_nums\n",
    "        batch_size_to_read (int): Chunk size for reading data from csv.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Submission-ready DataFrame containing predictions.\n",
    "    \"\"\"\n",
    "    val_month = 34\n",
    "    test = pd.read_csv('../data_cleaned/test.csv')\n",
    "    \n",
    "    data_test = test\n",
    "    PREDICTION = pd.DataFrame(columns=['shop_id','item_id','item_cnt_month'])\n",
    "    Y_true_l=[]\n",
    "    for X_val, Y_val in create_batch_val(batch_size, val_month, shop_item_pairs_in_dbn,batch_size_to_read):\n",
    "        shop_id = X_val.shop_id\n",
    "        item_id = X_val.item_id\n",
    "        if type(model) in [sklearn.linear_model._coordinate_descent.Lasso,\n",
    "                          SVC]:\n",
    "            \n",
    "            X_val.drop('shop_id', inplace=True, axis=1) \n",
    "            X_val.drop('item_category_id', inplace=True, axis=1) \n",
    "            X_val.drop('item_id', inplace=True, axis=1) \n",
    "            \n",
    "\n",
    "        elif type(model) ==LGBMRegressor:\n",
    "            \n",
    "            X_val = X_val.drop('item_id', axis=1)\n",
    "            X_val['shop_id'] = X_val['shop_id'].astype('category')\n",
    "            X_val['item_category_id'] = X_val['item_category_id'].astype('category')\n",
    "            X_val['city'] = X_val['city'].astype('category')\n",
    "            X_val['super_category'] = X_val['super_category'].astype('category')\n",
    "                    \n",
    "            pass\n",
    "\n",
    "        \n",
    "        if X_val is None:\n",
    "            continue\n",
    "            \n",
    "        Y_val = np.clip(Y_val,0,20)\n",
    "        \n",
    "        if X_val.empty:\n",
    "            print('None')\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        X_val = make_X_lag_format(X_val, val_month)\n",
    "\n",
    "        X_val=append_some_columns(X_val, val_month)\n",
    "        X_val = X_val[columns_order]\n",
    "\n",
    "        \n",
    "        y_val_pred=model.predict(X_val)\n",
    "        y_val_pred = np.clip(y_val_pred,0,20)\n",
    "        Y_true_l.append(Y_val)\n",
    "        \n",
    "        \n",
    "        app = pd.DataFrame({'item_id':item_id,'shop_id': shop_id, 'item_cnt_month':y_val_pred})\n",
    "        PREDICTION = pd.concat([PREDICTION, app],ignore_index=True)\n",
    "\n",
    " \n",
    "    \n",
    "    data_test = data_test.merge(PREDICTION,on=['shop_id','item_id'])[['ID','item_cnt_month']]\n",
    "    return data_test\n",
    "\n",
    "def create_submission_pipeline(merged, model,batch_size,shop_item_pairs_in_dbn, shop_item_pairs_WITH_PREV_in_dbn,batch_size_to_read,epochs):\n",
    "    \"\"\"\n",
    "    Pipeline for both training model and creating submission\n",
    "    Note: batch learning works properly only for LGBMRegressor. Using another model with batch_size<=len(shop_item_pairs_WITH_PREV_in_dbn[dbn]) will lead to incorrect results\n",
    "    Args:\n",
    "        merged (_type_): not used\n",
    "        model (object): Trained machine learning model.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        shop_item_pairs_in_dbn (pd.DataFrame): dataframe of cartesian products of (shop, item) for date_block_nums\n",
    "        shop_item_pairs_WITH_PREV_in_dbn ( np.array[np.array[np.array[int,int]]] ): array of accumulated cartesian products of (shop, item) for date_block_nums\n",
    "        batch_size_to_read (int): Chunk size for reading data from csv.\n",
    "        epochs (int): epocs\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Submission-ready DataFrame containing predictions.\n",
    "    \"\"\"\n",
    "    val_errors = []\n",
    "    \n",
    "    val_errors=[]\n",
    "\n",
    "    #print(f'model training on 34 started')\n",
    "    t1 = time.time()\n",
    "    model,columns_order = train_model(model,batch_size, 34, shop_item_pairs_WITH_PREV_in_dbn,batch_size_to_read,epochs, None)\n",
    "    t2 = time.time()\n",
    "    print('training model time,',t2-t1)\n",
    "    print('Feature importnaces in lgb:')\n",
    "    \n",
    "    print(model.feature_names_in_[np.argsort(model.feature_importances_)][::-1])\n",
    "    #print('n_estimators:', model.n_estimators_)\n",
    "    #print('submission creation started')\n",
    "    t1 = time.time()\n",
    "    data_test = create_submission(model,batch_size,columns_order, shop_item_pairs_in_dbn,batch_size_to_read)\n",
    "    t2 = time.time()\n",
    "    print('submission creation time,', t2-t1)\n",
    "\n",
    "    return data_test\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364950\n",
      "428871\n",
      "466086\n",
      "494493\n",
      "532909\n",
      "566259\n",
      "587979\n",
      "609623\n",
      "627192\n",
      "664844\n",
      "686985\n",
      "719696\n",
      "730116\n",
      "746129\n",
      "775024\n",
      "799403\n",
      "814628\n",
      "828506\n",
      "851457\n",
      "871899\n",
      "890066\n",
      "928598\n",
      "952398\n",
      "976804\n",
      "987057\n",
      "997953\n",
      "1013772\n",
      "1025692\n",
      "1035736\n",
      "1046582\n",
      "1055558\n",
      "1067461\n",
      "1080188\n",
      "1110590\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_train = pd.read_csv('../data_cleaned/data_train.csv')\n",
    "test = pd.read_csv('../data_cleaned/test.csv')\n",
    "test['date_block_num'] = 34\n",
    "data_train = pd.concat([data_train,test ], ignore_index=True).drop('ID', axis=1).fillna(0)\n",
    "\n",
    "\n",
    "shop_item_pairs_in_dbn, shop_item_pairs_WITH_PREV_in_dbn = prepare_past_ID_s_CARTESIAN(data_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_experiment(experiment_name):\n",
    "    \"\"\"\n",
    "    Retrieve the ID of an existing MLflow experiment or create a new one if it doesn't exist.\n",
    "\n",
    "    This function checks if an experiment with the given name exists within MLflow.\n",
    "    If it does, the function returns its ID. If not, it creates a new experiment\n",
    "    with the provided name and returns its ID.\n",
    "\n",
    "    Parameters:\n",
    "    - experiment_name (str): Name of the MLflow experiment.\n",
    "\n",
    "    Returns:\n",
    "    - str: ID of the existing or newly created MLflow experiment.\n",
    "    \"\"\"\n",
    "\n",
    "    if experiment := mlflow.get_experiment_by_name(experiment_name):\n",
    "        return experiment.experiment_id\n",
    "    else:\n",
    "        return mlflow.create_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "experiment_id=get_or_create_experiment('LGBM on month 33')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial,val_monthes):\n",
    "    with mlflow.start_run(experiment_id=experiment_id,nested=True):\n",
    "        lr = trial.suggest_float('lr', low=0.002, high = 0.07,log=True)\n",
    "        num_leaves = trial.suggest_int('num_leaves', low=32, high = 256,step=50)\n",
    "        n_estimators=trial.suggest_int('n_estimators', low=100, high = 400,step=100)\n",
    "\n",
    "        epochs=1\n",
    "        batch_size=3000000\n",
    "        batch_size_to_read=10**10\n",
    "        batches_for_training=1\n",
    "        print('validation started...')\n",
    "        params = defaultdict()\n",
    "        params['trial'] = trial\n",
    "        params['lr'] = lr\n",
    "        params['num_leaves'] = num_leaves\n",
    "        params['n_estimators'] = n_estimators\n",
    "        params['batch_size']=batch_size\n",
    "        params['batches_for_training']=batches_for_training\n",
    "\n",
    "        val_errors, val_preds = validate_ML(\n",
    "                                            params,\n",
    "                                            batch_size=batch_size,\n",
    "                                            val_monthes=val_monthes, \n",
    "                                            shop_item_pairs_in_dbn=shop_item_pairs_in_dbn,\n",
    "                                            shop_item_pairs_WITH_PREV_in_dbn=shop_item_pairs_WITH_PREV_in_dbn,\n",
    "                                            batch_size_to_read=batch_size_to_read,\n",
    "                                            epochs=epochs,\n",
    "                                            batches_for_training=batches_for_training)\n",
    "\n",
    "        print('lr:'+str(lr) + ' ' + str(val_errors) + '\\n')\n",
    "\n",
    "        mlflow.set_tag(\"Monthes\",val_monthes)\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"rmse\", np.mean(val_errors))\n",
    "        return np.mean(val_errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import logging\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-06 21:24:58,774] A new study created in RDB with name: no-name-9535c401-19bc-46e9-9e3c-2bc7ffc569a0\n",
      "/home/valeriy/python_envs/predict_future_sales/lib/python3.12/site-packages/optuna/distributions.py:704: UserWarning: The distribution is specified by [32, 256] and step=50, but the range is not divisible by `step`. It will be replaced by [32, 232].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation started...\n",
      "month 22 started\n",
      "month 10\n",
      "epoch, 0\n",
      "total batches, 3\n",
      "batch creation time [create_batch_train, 212], 625.5115249156952\n",
      "batch size 3644842\n",
      "batch 0 memory usage 1399.619328\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.493044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8843\n",
      "[LightGBM] [Info] Number of data points in the train set: 3644842, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score 0.129636\n"
     ]
    }
   ],
   "source": [
    "experiment_id=get_or_create_experiment('LGBM hyperparameter tuning on few monthes')\n",
    "val_monthes=[22,26,30,33]\n",
    "val_monthes_str = [str(i) for i in val_monthes]\n",
    "run_name=f'Run_{'_'.join(val_monthes_str)}_monthes'\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name=run_name, nested=True):\n",
    "   \n",
    "    study_name = \"val\"+'_'+'_'.join(val_monthes_str)  # Unique identifier of th\n",
    "    storage_name = \"sqlite:///{}.db\".format(study_name)\n",
    "    objective = partial(objective, val_monthes = val_monthes)\n",
    "    study = optuna.create_study(direction=\"minimize\", storage=storage_name, load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'numpy' from '/home/valeriy/python_envs/predict_future_sales/lib/python3.12/site-packages/numpy/__init__.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_future_sales",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
