{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae9c371-1eb5-469a-a385-e6cb58a2aa44",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbb6d503-8d7d-4314-a81b-13559178de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84f13414-bd09-49bd-aa70-b05a8efd23fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('../data_cleaned/data_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a80c07-e7f3-4e53-a0ad-2b7258c4c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data_cleaned/test.csv')\n",
    "test['date_block_num'] = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b6f5ddb-a854-477d-9b14-4c83987b88dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.concat([data_train,test ], ignore_index=True).drop('ID', axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9664f55-ca82-484a-a911-b883e8361f53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_price</th>\n",
       "      <th>item_cnt_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02.01.2013</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>22154</td>\n",
       "      <td>999.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03.01.2013</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>2552</td>\n",
       "      <td>899.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06.01.2013</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>2554</td>\n",
       "      <td>1709.05</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.01.2013</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>2555</td>\n",
       "      <td>1099.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.01.2013</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>2564</td>\n",
       "      <td>349.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3142680</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>18454</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3142681</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>16188</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3142682</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>15757</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3142683</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>19648</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3142684</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>969</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3142685 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date  date_block_num  shop_id  item_id  item_price  \\\n",
       "0        02.01.2013               0       59    22154      999.00   \n",
       "1        03.01.2013               0       25     2552      899.00   \n",
       "2        06.01.2013               0       25     2554     1709.05   \n",
       "3        15.01.2013               0       25     2555     1099.00   \n",
       "4        10.01.2013               0       25     2564      349.00   \n",
       "...             ...             ...      ...      ...         ...   \n",
       "3142680           0              34       45    18454        0.00   \n",
       "3142681           0              34       45    16188        0.00   \n",
       "3142682           0              34       45    15757        0.00   \n",
       "3142683           0              34       45    19648        0.00   \n",
       "3142684           0              34       45      969        0.00   \n",
       "\n",
       "         item_cnt_day  \n",
       "0                 1.0  \n",
       "1                 1.0  \n",
       "2                 1.0  \n",
       "3                 1.0  \n",
       "4                 1.0  \n",
       "...               ...  \n",
       "3142680           0.0  \n",
       "3142681           0.0  \n",
       "3142682           0.0  \n",
       "3142683           0.0  \n",
       "3142684           0.0  \n",
       "\n",
       "[3142685 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64238801-ebdd-4b87-a67f-62cf40287363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_past_ID_s(data_train):\n",
    "    data_train['shop_item'] = [tuple([shop, item]) for shop, item in zip(data_train['shop_id'], data_train['item_id'])]\n",
    "    #34 block contains A LOT more shop_item than others\n",
    "    shop_item_pairs_in_dbn = data_train.groupby('date_block_num')['shop_item'].apply(np.unique)\n",
    "    data_train = data_train.drop(['shop_item'], axis=1)\n",
    "    \n",
    "    shop_item_pairs_WITH_PREV_in_dbn = shop_item_pairs_in_dbn.copy()\n",
    "    \n",
    "    print(np.array(shop_item_pairs_WITH_PREV_in_dbn.index))\n",
    "    arr = np.array(shop_item_pairs_WITH_PREV_in_dbn.index)\n",
    "    for block in arr[arr>=0]:\n",
    "        if block == 0:\n",
    "            continue\n",
    "        shop_item_pairs_WITH_PREV_in_dbn[block] = np.unique(np.append(shop_item_pairs_WITH_PREV_in_dbn[block -1],\n",
    "                                                                      #shop_item_pairs_WITH_PREV_in_dbn[block]))\n",
    "                                                                      shop_item_pairs_in_dbn[block-1]))\n",
    "        print(len(shop_item_pairs_WITH_PREV_in_dbn[block]))\n",
    "\n",
    "    return shop_item_pairs_in_dbn, shop_item_pairs_WITH_PREV_in_dbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a411637-087a-4bb4-8aa3-b53ad381e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_past_ID_s_CARTESIAN(data_train):\n",
    "    data_train['shop_item'] = [tuple([shop, item]) for shop, item in zip(data_train['shop_id'], data_train['item_id'])]\n",
    "    #34 block contains A LOT more shop_item than others\n",
    "    shop_item_pairs_in_dbn = data_train.groupby('date_block_num')['shop_item'].apply(np.unique)\n",
    "    data_train = data_train.drop(['shop_item'], axis=1)\n",
    "    \n",
    "    shop_item_pairs_WITH_PREV_in_dbn = np.array([None] * len(shop_item_pairs_in_dbn))\n",
    "    \n",
    "    #print(np.array(shop_item_pairs_WITH_PREV_in_dbn.index))\n",
    "    \n",
    "\n",
    "    cartesians = []\n",
    "    for dbn in shop_item_pairs_in_dbn.index:\n",
    "        val = shop_item_pairs_in_dbn[dbn]\n",
    "\n",
    "        shops = np.unique(list(zip(*val))[0])\n",
    "        items = np.unique(list(zip(*val))[1])\n",
    "    \n",
    "        cartesian_product = np.random.permutation (np.array(np.meshgrid(shops, items)).T.reshape(-1, 2))\n",
    "        #print(cartesian_product)\n",
    "        cartesians.append(cartesian_product)\n",
    "        \n",
    "    \n",
    "    shop_item_pairs_WITH_PREV_in_dbn[0] = cartesians[0]\n",
    "    \n",
    "    for block in shop_item_pairs_in_dbn.index:\n",
    "        if block == 0:\n",
    "            continue\n",
    "        arr = np.append(shop_item_pairs_WITH_PREV_in_dbn[block - 1],\n",
    "                             cartesians[block - 1], axis=0)\n",
    "        \n",
    "        shop_item_pairs_WITH_PREV_in_dbn[block] = np.unique(arr, axis=0)\n",
    "        print(len(shop_item_pairs_WITH_PREV_in_dbn[block]))\n",
    "        \n",
    "    return shop_item_pairs_in_dbn, shop_item_pairs_WITH_PREV_in_dbn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6dc1798-6cad-4c98-b648-0807331d129f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34]\n",
      "63170\n",
      "97186\n",
      "126662\n",
      "146380\n",
      "162679\n",
      "178831\n",
      "192610\n",
      "205423\n",
      "216508\n",
      "228547\n",
      "240348\n",
      "255924\n",
      "263803\n",
      "271937\n",
      "281288\n",
      "290301\n",
      "299290\n",
      "307259\n",
      "314673\n",
      "322344\n",
      "330092\n",
      "338979\n",
      "349486\n",
      "362849\n",
      "368644\n",
      "374512\n",
      "380928\n",
      "386411\n",
      "391327\n",
      "396282\n",
      "401230\n",
      "406603\n",
      "411762\n",
      "418882\n"
     ]
    }
   ],
   "source": [
    "shop_item_pairs_in_dbn, shop_item_pairs_WITH_PREV_in_dbn = prepare_past_ID_s(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4b68fee-1c1a-491d-899b-8aa7798e0efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75bdcebb-258f-4bb6-8fe7-4cc998ae11b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60ac9cfe-7563-457e-be30-b28799b2d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X_lag_format(data, dbn):\n",
    "    \"\"\"\n",
    "    transform X to lag format\n",
    "    columns with dbn in names become lag_0, dbn-1 - lag_1 etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    lag_cols = defaultdict()\n",
    "    for col in data.columns:\n",
    "        splitted = col.split('$')\n",
    "        \n",
    "        if len(splitted) == 1:\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        lag_cols[col] = splitted[0] + '_lag_' + str(dbn - int(splitted[1]))\n",
    "\n",
    "    #print(lag_cols)\n",
    "    data = data.rename(columns=dict(lag_cols))\n",
    "    #print(data.columns)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7621cf0-3ab0-4cb6-aa66-e1d3f4cd817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train(data, valid ):\n",
    "    \"\"\"\n",
    "    returns one batch of merged data with required IDs from valid\n",
    "    \"\"\"\n",
    "    #print(data)\n",
    "    valid_shop_item = valid\n",
    "    valid_shop_item = list(zip(*valid_shop_item))\n",
    "    df = pd.DataFrame({'item_id':valid_shop_item[1],'shop_id':valid_shop_item[0]} )\n",
    "    data = df.merge(data, on=['shop_id','item_id'], how='left').fillna(0)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e601036-36bd-49a8-9c88-926b1c817062",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_val(data, valid ):\n",
    "    \"\"\"\n",
    "    returns one batch of merged data with required IDs from valid\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame({'item_id':valid[:,1],'shop_id':valid[:,0]} )\n",
    "    data = df.merge(data, on=['shop_id','item_id'], how='left').fillna(0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9891c579-9b0d-43fc-b3e4-a83b6f1d55d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5701d92f-5772-4664-8d54-8d38e33b7231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_train_LSTM(data, valid, dbn):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    train = prepare_train (data, valid)\n",
    "    lag_cols = []\n",
    "    for col in data.columns:\n",
    "\n",
    "        splitted = col.split('$')\n",
    "        if len(splitted)==1:\n",
    "            lag_cols.append(col)\n",
    "            continue\n",
    "        #if 'shop_item_cnt' not in col:\n",
    "        #    continue\n",
    "            \n",
    "        for db in range(0,dbn-1):\n",
    "            \n",
    "            if db == int(splitted[1]):\n",
    "                lag_cols.append(col)\n",
    "\n",
    "    X = train[lag_cols]\n",
    "\n",
    "    \n",
    "    Y = train[f'shop_item_cnt${dbn-1}']\n",
    "    \n",
    "    return X, Y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf0a5fe-2a8e-4744-a045-da54df2ea451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1ba88ce-aa62-4d64-93e2-c0000283582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_validation_LSTM(data, valid, dbn):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    test = prepare_val (data, valid)\n",
    "    \n",
    "    lag_cols = []\n",
    "    \n",
    "    for col in test.columns:\n",
    "        \n",
    "        splitted = col.split('$')\n",
    "            \n",
    "        if len(splitted) == 1:\n",
    "            lag_cols.append(col)\n",
    "            continue\n",
    "        #if 'shop_item_cnt' not in col:\n",
    "        #    continue\n",
    "        for db in range(1,dbn):\n",
    "            \n",
    "            if db == int(splitted[1]):\n",
    "                #print(db, int(''.join(re.findall(r'\\d+', col))))\n",
    "                lag_cols.append(col)\n",
    "\n",
    "    X = test[lag_cols]\n",
    "    Y = test[f'shop_item_cnt${dbn}']\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7667a652-2001-4d72-b6e2-4f23211b2cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.permutation([[1,2],[3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd2033b2-0c53-41b6-be39-812be63a909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_train(merged, batch_size, dbn):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    #merged = pd.read_csv('data/merged.csv', chunksize=500000)\n",
    "    #merged = pd.read_csv('data/merged.csv')\n",
    "    train = np.random.permutation (shop_item_pairs_WITH_PREV_in_dbn[dbn])\n",
    "    #train = shop_item_pairs_WITH_PREV_in_dbn[dbn]\n",
    "    chunck_num = (len(train)  // batch_size) + 1\n",
    "    \n",
    "    for idx in range(chunck_num):#split shop_item_pairs_WITH_PREV_in_dbn into chuncks\n",
    "        #for chunck in merged:#split merged into chuncks\n",
    "        train_ret = prepare_data_train_LSTM(merged,train[idx*batch_size:(idx+1)*batch_size], dbn)\n",
    "       \n",
    "        if  train_ret[0].empty:\n",
    "            yield [None, None]\n",
    "        \n",
    "        yield train_ret#, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1671dc4a-f498-4b79-95a6-2832c880114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_val(merged, batch_size, dbn):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    #merged = pd.read_csv('data/merged.csv', chunksize=500000) - (DOESNT WORK PROPERLY))))) - use it if merged doesnt fit memory\n",
    "    #merged = pd.read_csv('data/merged.csv')\n",
    "    val = shop_item_pairs_in_dbn[dbn]\n",
    "\n",
    "    shops = np.unique(list(zip(*val))[0])\n",
    "    items = np.unique(list(zip(*val))[1])\n",
    "\n",
    "    cartesian_product = np.random.permutation (np.array(np.meshgrid(shops, items)).T.reshape(-1, 2))\n",
    "    \n",
    "    chunck_num = (len(cartesian_product)  // batch_size) + 1\n",
    "    for idx in range(chunck_num):\n",
    "        #for chunck in merged:\n",
    "        train_ret = prepare_data_validation_LSTM(merged,cartesian_product[idx*batch_size:(idx+1)*batch_size], dbn)\n",
    "        #When in batches idx no elements that are in (shop, item) in batch of merged\n",
    "        if  train_ret[0].empty:\n",
    "            \n",
    "            yield [None, None]\n",
    "        #print(len(train_ret))\n",
    "        \n",
    "        yield train_ret#, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a4f80d7-d247-49d2-b393-2e90ec6ffefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "class CustomLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim=1, hidden_dim=64,hidden_linear=64,  tagset_size=1, N_LEVELS=None, device=None):\n",
    "        super(CustomLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim,\n",
    "                            hidden_size = self.hidden_dim,\n",
    "                            batch_first=True,\n",
    "                            proj_size=tagset_size,\n",
    "                            num_layers=N_LEVELS,\n",
    "                           device=device)\n",
    "\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(tagset_size, hidden_linear),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_linear, 1)\n",
    "        ).to(device)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        lstm_out, _ = self.lstm(data)\n",
    "\n",
    "        \n",
    "        linear_out = self.linear_layers(lstm_out)\n",
    "        \n",
    "        return linear_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f4b5ebc-6062-4d7d-a504-3a6cf39a0d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_COLUMNS  = ['shop_item_cnt','shop_item_price','shop_category_cnt','shop_item_date_block_num_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98e7fd96-5e34-480e-ae6b-d95d69340de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preapre_X_LSTM_format(X_train, dbn, device):\n",
    "    #Batch come here\n",
    "    #one row - one train example\n",
    "    cols=[]\n",
    "    data = defaultdict(list)\n",
    "    for col in X_train.columns:\n",
    "        if 'change' in col:\n",
    "            continue\n",
    "        #if 'avg_item_price' in col:\n",
    "        #   cols.append(col)\n",
    "        #   continue\n",
    "\n",
    "        if 'shop_item_cnt' in col:\n",
    "            cols.append(col)\n",
    "            continue\n",
    "\n",
    "        \n",
    "            \n",
    "        \"\"\"\n",
    "        if not col[-1].isdigit():\n",
    "            continue\n",
    "\n",
    "        \n",
    "\n",
    "        if not any(c in col for c in SELECTED_COLUMNS) :\n",
    "            continue\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    #print('COLUMNS',list(X_train[cols].columns))\n",
    "    #print(X_train[cols].columns)\n",
    "    X_train = X_train[cols].values.reshape(len(X_train), dbn-1, -1)\n",
    "    #print(X_train)\n",
    "    return torch.tensor(X_train, device=device).to(dtype=torch.float32)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9558394d-f844-4aba-a6a2-a202df8a282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preapre_Y_LSTM_format(Y_train, device):\n",
    "    return torch.tensor(Y_train, device=device).to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "365fc686-e540-4521-a3e3-afb5aad34203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_some_columns(X_train, dbn):\n",
    "    X_train['date_block_num'] = dbn\n",
    "    X_train['month'] = dbn%12\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b281841f-bdb0-4be9-bd7d-a39f6a2e0fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15884b7b-68fd-4300-95dd-626f0466a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "183e5293-d9c6-4ae6-9832-922f4bfd6eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e477d0f8-6c93-49b3-8055-6303c8d4b360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96784c6e-0884-4c8a-bd0e-89734a536504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(model,optimizer,loss_fn,   merged,batch_size, val_month, epochs=20):\n",
    "    \n",
    "    first=True\n",
    "    rmse = 0\n",
    "    c=0\n",
    "    \n",
    "    preds_l=[]\n",
    "    y_true_l=[]\n",
    "\n",
    "    grads = []\n",
    "    for X_train,Y_train  in create_batch_train(merged,batch_size, val_month):\n",
    "        \n",
    "        if X_train is None:\n",
    "            print('None')\n",
    "            continue\n",
    "        Y_train = np.clip(Y_train,0,20)\n",
    "        \n",
    "        if X_train.empty:\n",
    "            print('None')\n",
    "            continue\n",
    "        X_train = make_X_lag_format(X_train, val_month-1)\n",
    "        X_train=preapre_X_LSTM_format(X_train, val_month-1, device)\n",
    "        Y_train = preapre_Y_LSTM_format(Y_train, device)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(X_train)[:,-1,:]\n",
    "        \n",
    "        preds_l.append(torch.squeeze(preds))\n",
    "        y_true_l.append(torch.squeeze(Y_train))\n",
    "        \n",
    "        loss_train = loss_fn(torch.squeeze(preds), \n",
    "                             torch.squeeze(Y_train))\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss_train.backward()\n",
    "        \n",
    "        total_norm = torch.max( torch.stack([p.grad.detach().abs().max() for p in model.parameters()]) )\n",
    "        grads.append(total_norm)\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        c+=1\n",
    "\n",
    "    preds_l = torch.concat(preds_l)\n",
    "    y_true_l = torch.concat(y_true_l)\n",
    "    print('mean of max grad,',torch.mean(torch.tensor(grads)))\n",
    "    with torch.no_grad():\n",
    "        metric = torch.sqrt(loss_fn(torch.clamp(preds_l,0,20), y_true_l))\n",
    "        #print(pd.DataFrame(torch.clamp(y_true_l,0,20).numpy(force=True)).describe())\n",
    "        \n",
    "    return model, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dee3eaf3-559a-4b78-bfda-9ef3d573fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_lstm(model,merged,batch_size, val_month):\n",
    "    val_error = 0\n",
    "    c=0\n",
    "    val_preds=[]\n",
    "    preds_l=[]\n",
    "    y_true_l=[]\n",
    "    #create_batch_train(merged,batch_size, val_month) - return train set, where Y_val\n",
    "    #is shop_item_cnt_month{val_month}\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for X_val, Y_val in create_batch_val(merged,batch_size, val_month):#but then cartesian product used\n",
    "        \n",
    "        if X_val is None:\n",
    "            continue\n",
    "        if X_val.empty:\n",
    "            print('None')\n",
    "            continue\n",
    "        Y_val = np.clip(Y_val,0,20)        \n",
    "        X_val = make_X_lag_format(X_val, val_month)\n",
    "        \n",
    "        \n",
    "        X_val=preapre_X_LSTM_format(X_val, val_month, device)\n",
    "        Y_val = preapre_Y_LSTM_format(Y_val, device)\n",
    "\n",
    "        #print(X_val.size())\n",
    "        #X_val = torch.nn.functional.normalize(X_val, p=1, dim=1)\n",
    "        #X_val /= torch.max(X_val, dim=1, keepdim=True)[0]\n",
    "        #X_val = torch.nan_to_num(X_val, nan=0.0)\n",
    "        #X_val /= 20.0\n",
    "        #Y_val /= 20.0\n",
    "        #X_val = torch.nn.functional.normalize(X_val, p=1, dim=1)\n",
    "        #Y_val = torch.nn.functional.normalize(Y_val, p=1, dim=1)\n",
    "        if c==0:\n",
    "            pass\n",
    "            #print('train columns',X_val.columns)\n",
    "            #print(X_val.date_block_num)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = model(X_val)[:,-1,:]\n",
    "            loss_rmse = torch.sqrt(loss_fn(torch.squeeze(y_val_pred),\n",
    "                                                 torch.squeeze(Y_val)))\n",
    "            \n",
    "            \n",
    "            preds_l.append(torch.squeeze(y_val_pred))\n",
    "            y_true_l.append(torch.squeeze(Y_val))\n",
    "            val_preds.append(y_val_pred)\n",
    "            c+=1\n",
    "            \n",
    "    preds_l = torch.concat(preds_l)\n",
    "    y_true_l = torch.concat(y_true_l)\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        metric = torch.sqrt(loss_fn(torch.clamp(preds_l,0,20)*20, y_true_l*20))\n",
    "        #print(pd.DataFrame(torch.clamp(y_true_l,0,20).numpy(force=True)).describe())\n",
    "        \n",
    "            \n",
    "    return preds_l, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1561f7b-e24f-44c7-a404-70412fc936b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_LSTM(merged, epochs=None,start_val_month =None):\n",
    "    \"\"\"\n",
    "    Function for validating model\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    val_errors = []\n",
    "    batch_size =20000\n",
    "    val_preds=[]\n",
    "    lr = 0.003\n",
    "    step_size=40\n",
    "    gamma=0.1\n",
    "    \n",
    "    for val_month in range(start_val_month, 34):\n",
    "\n",
    "        \n",
    "        loss_fn = nn.MSELoss()\n",
    "        print('date_block_num', val_month)\n",
    "        print('month', val_month%12)\n",
    "        \n",
    "        model = CustomLSTM(embedding_dim=EMBEDDING_DIM,\n",
    "                   hidden_dim=HIDDEN_DIM,\n",
    "                   hidden_linear=64,\n",
    "                   tagset_size=TARGET_SIZE,\n",
    "                   N_LEVELS=N_LEVELS,\n",
    "                   device=device)\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.8, 0.999))\n",
    "        scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "        \n",
    "            \n",
    "            \n",
    "            model,train_error = train_lstm(model,\n",
    "                                           optimizer,\n",
    "                                           loss_fn, \n",
    "                                           merged,\n",
    "                                           batch_size, \n",
    "                                           val_month)\n",
    "            \n",
    "            scheduler.step()\n",
    "            if epoch % 3 == 0:\n",
    "                val_pred, val_error = validate_lstm(model,\n",
    "                                                    merged,\n",
    "                                                    batch_size,\n",
    "                                                    val_month)\n",
    "\n",
    "                print('prediction')\n",
    "                print('mean', torch.mean(val_pred))\n",
    "                print('max', torch.max(val_pred))\n",
    "                print('min', torch.min(val_pred))\n",
    "                print('std', torch.std(val_pred))\n",
    "                print('mean train rmse on epoch', epoch,':',train_error )\n",
    "                print('mean val rmse on epoch', epoch,':',val_error )\n",
    "                \n",
    "\n",
    "            if epoch % 13 == 0:\n",
    "                print('lr:',scheduler.get_last_lr())\n",
    "                \n",
    "            \n",
    "            val_errors.append(val_error)\n",
    "            val_preds.append(val_pred)\n",
    "        \n",
    "\n",
    "    return val_errors, val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a26ba38a-0f74-4e05-8de3-8748687c12e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chunksize = 50000\n",
    "l=[]\n",
    "with pd.read_csv('data/merged.csv', chunksize=chunksize) as reader:\n",
    "    for chunk in reader:\n",
    "        l.append(chunk)\n",
    "\n",
    "merged = pd.concat(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad050efb-d0c4-4ce4-a1d7-f804dad378fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ca58af0-caee-4002-ae31-ce73964d3f33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=1\n",
    "HIDDEN_DIM=128#512\n",
    "TARGET_SIZE=2\n",
    "N_LEVELS=1\n",
    "\n",
    "epochs=500\n",
    "start_val_month=22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04ac5d00-4706-4c8e-9dc0-d1c72c039151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62938/2933082444.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(-inf)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "886bad01-f81b-4606-90b9-4dcea9fd4d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valeriy/python_envs/predict_future_sales/lib/python3.12/site-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: divide by zero encountered in log\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def normalize(data):\n",
    "    data[[col for col in merged.columns if 'cnt' in col ]] = np.clip(data[[col for col in merged.columns if 'cnt' in col ]],0,20)\n",
    "    data[[col for col in merged.columns if 'price' in col ]] = \\\n",
    "    np.clip(np.nan_to_num( np.log(data[[col for col in merged.columns if 'price' in col ]]),  posinf=0, neginf=0),\n",
    "            0,\n",
    "            10000) / 10000\n",
    "    data[[col for col in merged.columns if 'cnt' in col ]] /= 20\n",
    "normalize(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0f1941f-4be5-4ff5-acd2-55f10b74540d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_block_num 22\n",
      "month 10\n",
      "mean of max grad, tensor(0.0899)\n",
      "prediction\n",
      "mean tensor(-0.0050, device='cuda:0')\n",
      "max tensor(0.0008, device='cuda:0')\n",
      "min tensor(-0.0050, device='cuda:0')\n",
      "std tensor(0.0003, device='cuda:0')\n",
      "mean train rmse on epoch 0 : tensor(0.0679, device='cuda:0')\n",
      "mean val rmse on epoch 0 : tensor(1.3628, device='cuda:0')\n",
      "lr: [0.003]\n",
      "mean of max grad, tensor(0.0125)\n",
      "mean of max grad, tensor(0.0026)\n",
      "mean of max grad, tensor(0.0023)\n",
      "prediction\n",
      "mean tensor(0.0116, device='cuda:0')\n",
      "max tensor(1.0224, device='cuda:0')\n",
      "min tensor(0.0024, device='cuda:0')\n",
      "std tensor(0.0373, device='cuda:0')\n",
      "mean train rmse on epoch 3 : tensor(0.0570, device='cuda:0')\n",
      "mean val rmse on epoch 3 : tensor(1.1223, device='cuda:0')\n",
      "mean of max grad, tensor(0.0012)\n",
      "mean of max grad, tensor(0.0026)\n",
      "mean of max grad, tensor(0.0013)\n",
      "prediction\n",
      "mean tensor(0.0151, device='cuda:0')\n",
      "max tensor(0.8820, device='cuda:0')\n",
      "min tensor(0.0076, device='cuda:0')\n",
      "std tensor(0.0365, device='cuda:0')\n",
      "mean train rmse on epoch 6 : tensor(0.0524, device='cuda:0')\n",
      "mean val rmse on epoch 6 : tensor(1.1184, device='cuda:0')\n",
      "mean of max grad, tensor(0.0035)\n",
      "mean of max grad, tensor(0.0023)\n",
      "mean of max grad, tensor(0.0019)\n",
      "prediction\n",
      "mean tensor(0.0135, device='cuda:0')\n",
      "max tensor(0.8980, device='cuda:0')\n",
      "min tensor(0.0063, device='cuda:0')\n",
      "std tensor(0.0361, device='cuda:0')\n",
      "mean train rmse on epoch 9 : tensor(0.0523, device='cuda:0')\n",
      "mean val rmse on epoch 9 : tensor(1.1208, device='cuda:0')\n",
      "mean of max grad, tensor(0.0026)\n",
      "mean of max grad, tensor(0.0020)\n",
      "mean of max grad, tensor(0.0020)\n",
      "prediction\n",
      "mean tensor(0.0172, device='cuda:0')\n",
      "max tensor(0.9864, device='cuda:0')\n",
      "min tensor(0.0095, device='cuda:0')\n",
      "std tensor(0.0382, device='cuda:0')\n",
      "mean train rmse on epoch 12 : tensor(0.0523, device='cuda:0')\n",
      "mean val rmse on epoch 12 : tensor(1.1204, device='cuda:0')\n",
      "mean of max grad, tensor(0.0040)\n",
      "lr: [0.003]\n",
      "mean of max grad, tensor(0.0029)\n",
      "mean of max grad, tensor(0.0041)\n",
      "prediction\n",
      "mean tensor(0.0159, device='cuda:0')\n",
      "max tensor(0.9551, device='cuda:0')\n",
      "min tensor(0.0087, device='cuda:0')\n",
      "std tensor(0.0364, device='cuda:0')\n",
      "mean train rmse on epoch 15 : tensor(0.0523, device='cuda:0')\n",
      "mean val rmse on epoch 15 : tensor(1.1188, device='cuda:0')\n",
      "mean of max grad, tensor(0.0034)\n",
      "mean of max grad, tensor(0.0022)\n",
      "mean of max grad, tensor(0.0025)\n",
      "prediction\n",
      "mean tensor(0.0170, device='cuda:0')\n",
      "max tensor(0.9791, device='cuda:0')\n",
      "min tensor(0.0098, device='cuda:0')\n",
      "std tensor(0.0367, device='cuda:0')\n",
      "mean train rmse on epoch 18 : tensor(0.0522, device='cuda:0')\n",
      "mean val rmse on epoch 18 : tensor(1.1195, device='cuda:0')\n",
      "mean of max grad, tensor(0.0040)\n",
      "mean of max grad, tensor(0.0017)\n",
      "mean of max grad, tensor(0.0021)\n",
      "prediction\n",
      "mean tensor(0.0178, device='cuda:0')\n",
      "max tensor(0.9887, device='cuda:0')\n",
      "min tensor(0.0104, device='cuda:0')\n",
      "std tensor(0.0367, device='cuda:0')\n",
      "mean train rmse on epoch 21 : tensor(0.0522, device='cuda:0')\n",
      "mean val rmse on epoch 21 : tensor(1.1191, device='cuda:0')\n",
      "mean of max grad, tensor(0.0040)\n",
      "mean of max grad, tensor(0.0018)\n",
      "mean of max grad, tensor(0.0035)\n",
      "prediction\n",
      "mean tensor(0.0116, device='cuda:0')\n",
      "max tensor(0.9234, device='cuda:0')\n",
      "min tensor(0.0050, device='cuda:0')\n",
      "std tensor(0.0343, device='cuda:0')\n",
      "mean train rmse on epoch 24 : tensor(0.0522, device='cuda:0')\n",
      "mean val rmse on epoch 24 : tensor(1.1227, device='cuda:0')\n",
      "mean of max grad, tensor(0.0036)\n",
      "mean of max grad, tensor(0.0024)\n",
      "lr: [0.003]\n",
      "mean of max grad, tensor(0.0029)\n",
      "prediction\n",
      "mean tensor(0.0147, device='cuda:0')\n",
      "max tensor(0.9199, device='cuda:0')\n",
      "min tensor(0.0081, device='cuda:0')\n",
      "std tensor(0.0341, device='cuda:0')\n",
      "mean train rmse on epoch 27 : tensor(0.0522, device='cuda:0')\n",
      "mean val rmse on epoch 27 : tensor(1.1194, device='cuda:0')\n",
      "mean of max grad, tensor(0.0032)\n",
      "mean of max grad, tensor(0.0029)\n",
      "mean of max grad, tensor(0.0020)\n",
      "prediction\n",
      "mean tensor(0.0113, device='cuda:0')\n",
      "max tensor(1.0228, device='cuda:0')\n",
      "min tensor(0.0043, device='cuda:0')\n",
      "std tensor(0.0367, device='cuda:0')\n",
      "mean train rmse on epoch 30 : tensor(0.0521, device='cuda:0')\n",
      "mean val rmse on epoch 30 : tensor(1.1213, device='cuda:0')\n",
      "mean of max grad, tensor(0.0021)\n",
      "mean of max grad, tensor(0.0017)\n",
      "mean of max grad, tensor(0.0037)\n",
      "prediction\n",
      "mean tensor(0.0153, device='cuda:0')\n",
      "max tensor(1.0080, device='cuda:0')\n",
      "min tensor(0.0085, device='cuda:0')\n",
      "std tensor(0.0358, device='cuda:0')\n",
      "mean train rmse on epoch 33 : tensor(0.0522, device='cuda:0')\n",
      "mean val rmse on epoch 33 : tensor(1.1177, device='cuda:0')\n",
      "mean of max grad, tensor(0.0020)\n",
      "mean of max grad, tensor(0.0023)\n",
      "mean of max grad, tensor(0.0019)\n",
      "prediction\n",
      "mean tensor(0.0155, device='cuda:0')\n",
      "max tensor(1.0583, device='cuda:0')\n",
      "min tensor(0.0081, device='cuda:0')\n",
      "std tensor(0.0380, device='cuda:0')\n",
      "mean train rmse on epoch 36 : tensor(0.0521, device='cuda:0')\n",
      "mean val rmse on epoch 36 : tensor(1.1184, device='cuda:0')\n",
      "mean of max grad, tensor(0.0027)\n",
      "mean of max grad, tensor(0.0038)\n",
      "mean of max grad, tensor(0.0025)\n",
      "prediction\n",
      "mean tensor(0.0115, device='cuda:0')\n",
      "max tensor(1.0196, device='cuda:0')\n",
      "min tensor(0.0046, device='cuda:0')\n",
      "std tensor(0.0362, device='cuda:0')\n",
      "mean train rmse on epoch 39 : tensor(0.0521, device='cuda:0')\n",
      "mean val rmse on epoch 39 : tensor(1.1205, device='cuda:0')\n",
      "lr: [0.00030000000000000003]\n",
      "mean of max grad, tensor(0.0021)\n",
      "mean of max grad, tensor(0.0008)\n",
      "mean of max grad, tensor(0.0008)\n",
      "prediction\n",
      "mean tensor(0.0138, device='cuda:0')\n",
      "max tensor(1.0168, device='cuda:0')\n",
      "min tensor(0.0064, device='cuda:0')\n",
      "std tensor(0.0364, device='cuda:0')\n",
      "mean train rmse on epoch 42 : tensor(0.0521, device='cuda:0')\n",
      "mean val rmse on epoch 42 : tensor(1.1177, device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m val_errors, val_preds \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_LSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_val_month\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 35\u001b[0m, in \u001b[0;36mvalidate_LSTM\u001b[0;34m(merged, epochs, start_val_month)\u001b[0m\n\u001b[1;32m     29\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m StepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39mstep_size, gamma\u001b[38;5;241m=\u001b[39mgamma)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 35\u001b[0m     model,train_error \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mmerged\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mval_month\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[30], line 11\u001b[0m, in \u001b[0;36mtrain_lstm\u001b[0;34m(model, optimizer, loss_fn, merged, batch_size, val_month, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m y_true_l\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     10\u001b[0m grads \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 11\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcreate_batch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_month\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 13\u001b[0m, in \u001b[0;36mcreate_batch_train\u001b[0;34m(merged, batch_size, dbn)\u001b[0m\n\u001b[1;32m      9\u001b[0m chunck_num \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(train)  \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(chunck_num):\u001b[38;5;66;03m#split shop_item_pairs_WITH_PREV_in_dbn into chuncks\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#for chunck in merged:#split merged into chuncks\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     train_ret \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data_train_LSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m  train_ret[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m, in \u001b[0;36mprepare_data_train_LSTM\u001b[0;34m(data, valid, dbn)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_data_train_LSTM\u001b[39m(data, valid, dbn):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     train \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     lag_cols \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m, in \u001b[0;36mprepare_train\u001b[0;34m(data, valid)\u001b[0m\n\u001b[1;32m      7\u001b[0m valid_shop_item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mvalid_shop_item))\n\u001b[1;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_id\u001b[39m\u001b[38;5;124m'\u001b[39m:valid_shop_item[\u001b[38;5;241m1\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshop_id\u001b[39m\u001b[38;5;124m'\u001b[39m:valid_shop_item[\u001b[38;5;241m0\u001b[39m]} )\n\u001b[0;32m----> 9\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mshop_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mitem_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/python_envs/predict_future_sales/lib/python3.12/site-packages/pandas/core/frame.py:10832\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m  10813\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m  10814\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m  10815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10828\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m  10829\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m  10830\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[0;32m> 10832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  10833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10841\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10842\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python_envs/predict_future_sales/lib/python3.12/site-packages/pandas/core/reshape/merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[1;32m    156\u001b[0m         left_df,\n\u001b[1;32m    157\u001b[0m         right_df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/python_envs/predict_future_sales/lib/python3.12/site-packages/pandas/core/reshape/merge.py:800\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft\u001b[38;5;241m.\u001b[39m_drop_labels_or_levels(left_drop)\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m right_drop:\n\u001b[0;32m--> 800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_labels_or_levels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright_drop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_require_matching_dtypes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys)\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tolerance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys)\n",
      "File \u001b[0;32m~/python_envs/predict_future_sales/lib/python3.12/site-packages/pandas/core/generic.py:1989\u001b[0m, in \u001b[0;36mNDFrame._drop_labels_or_levels\u001b[0;34m(self, keys, axis)\u001b[0m\n\u001b[1;32m   1987\u001b[0m     \u001b[38;5;66;03m# Handle dropping columns labels\u001b[39;00m\n\u001b[1;32m   1988\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels_to_drop:\n\u001b[0;32m-> 1989\u001b[0m         \u001b[43mdropped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels_to_drop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1991\u001b[0m     \u001b[38;5;66;03m# Handle dropping column levels\u001b[39;00m\n\u001b[1;32m   1992\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m levels_to_drop:\n",
      "File \u001b[0;32m~/python_envs/predict_future_sales/lib/python3.12/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python_envs/predict_future_sales/lib/python3.12/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/python_envs/predict_future_sales/lib/python3.12/site-packages/pandas/core/generic.py:4869\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4866\u001b[0m     new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   4868\u001b[0m bm_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m axis_num \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 4869\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4872\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4875\u001b[0m \u001b[43m    \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4876\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4877\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_mgr, axes\u001b[38;5;241m=\u001b[39mnew_mgr\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   4878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/python_envs/predict_future_sales/lib/python3.12/site-packages/pandas/core/internals/managers.py:680\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested axis not found in manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slice_take_blocks_ax0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[1;32m    689\u001b[0m             indexer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[1;32m    696\u001b[0m     ]\n",
      "File \u001b[0;32m~/python_envs/predict_future_sales/lib/python3.12/site-packages/pandas/core/internals/managers.py:843\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[0;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[1;32m    841\u001b[0m                     blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[1;32m    842\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 843\u001b[0m                 nb \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m                 blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n",
      "File \u001b[0;32m~/python_envs/predict_future_sales/lib/python3.12/site-packages/pandas/core/internals/blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[0;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[0;32m~/python_envs/predict_future_sales/lib/python3.12/site-packages/pandas/core/array_algos/take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[1;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python_envs/predict_future_sales/lib/python3.12/site-packages/pandas/core/array_algos/take.py:162\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[1;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[1;32m    161\u001b[0m )\n\u001b[0;32m--> 162\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[1;32m    165\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val_errors, val_preds = validate_LSTM(merged, epochs, start_val_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e77e08-9600-4d76-b9a7-249cc68b715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.1211(best 1.1169, epoch 21),\n",
    "1.3282, \n",
    "1.0123(best 0.9, epoch 9),\n",
    "0.8688,\n",
    "0.8658, \n",
    "0.9597, \n",
    "0.8992,\n",
    "0.8306,\n",
    "0.7505,\n",
    "0.8229,\n",
    "0.9836   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "176c103a-1182-4b5b-8c15-b54ed93b7794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9493363636363639)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np. mean([1.1211,  1.3282,  1.0123, 0.8688, 0.8658, 0.9597, 0.8992, 0.8306,0.7505,0.8229,0.9836]   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233bbd30-31ab-4638-b7d4-8d38b3a1c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "errs = defaultdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1537f324-437f-4ada-963b-3d3bb4952854",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2092d6d9-e202-4b0e-af60-d56de75fc05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = torch.tensor(val_errors).numpy(force=True).reshape(12,-1)\n",
    "np.array(errors[:,-1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d4781d-35ad-44dc-81e8-b6f6f0648ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(model,merged,batch_size):\n",
    "    val_month = 34\n",
    "    test = pd.read_csv('../data_cleaned/test.csv')\n",
    "    \n",
    "    data_test = test\n",
    "    PREDICTION = pd.DataFrame(columns=['shop_id','item_id','item_cnt_month'])\n",
    "    \n",
    "    print('date_block_num', val_month)\n",
    "    print('month', val_month%12)\n",
    "    for X_val, Y_val in create_batch_val(merged,batch_size, val_month):#but then cartesian product used\n",
    "        shops= X_val.shop_id\n",
    "        items = X_val.item_id\n",
    "        if X_val is None:\n",
    "            continue\n",
    "        if X_val.empty:\n",
    "            print('None')\n",
    "            continue\n",
    "        Y_val = np.clip(Y_val,0,20)        \n",
    "        X_val = make_X_lag_format(X_val, val_month)\n",
    "        \n",
    "        \n",
    "        X_val=preapre_X_LSTM_format(X_val, val_month, device)\n",
    "        Y_val = preapre_Y_LSTM_format(Y_val, device)\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_val_pred = model(X_val)[:,-1,:]\n",
    "        \n",
    "            app = pd.DataFrame({'item_id':items,\n",
    "                                'shop_id': shops,\n",
    "                                'item_cnt_month':y_val_pred.numpy(force=True).flatten()})\n",
    "            PREDICTION = pd.concat([PREDICTION, app],ignore_index=True)\n",
    "\n",
    "\n",
    "    \n",
    "    data_test = data_test.merge(PREDICTION,on=['shop_id','item_id'])[['ID','item_cnt_month']]\n",
    "    return data_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c0f007-65ae-41d0-910d-a9a17b9f3f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_pipeline(merged):\n",
    "    batch_size=10000\n",
    "    model = CustomLSTM(EMBEDDING_DIM, HIDDEN_DIM, TARGET_SIZE, N_LEVELS,device=device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    val_month=34\n",
    "    epochs=50\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=13, gamma=0.3)\n",
    "    for epoch in range(epochs):\n",
    "        print('training epoch',epoch)\n",
    "        model,columns_order = train_lstm(model,optimizer,loss_fn,   merged,batch_size, val_month, epochs=20)\n",
    "        scheduler.step()\n",
    "    \n",
    "    \n",
    "    data_test = create_submission(model,merged,batch_size)\n",
    "\n",
    "    return data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ccfaf1-8c82-4383-8195-c6f991a26269",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission = create_submission_pipeline(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b4fe1a-e885-4c5b-9d4f-4f378356983f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "994efbb0-d233-4959-a865-a33cfed99062",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c769015-a9d8-4870-9774-fb02cfb77241",
   "metadata": {},
   "outputs": [],
   "source": [
    "12 epochs\n",
    "validation - 0.9546194\n",
    "test - 1.04\n",
    "35 epochs\n",
    "test - 1.03"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
